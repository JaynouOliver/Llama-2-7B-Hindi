{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "%pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# We have to check which Torch version for Xformers (2.3 -> 0.0.27)\n",
    "from torch import __version__; from packaging.version import Version as V\n",
    "xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n",
    "%pip install --no-deps {xformers} trl peft accelerate bitsandbytes triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: BNB_CUDA_VERSION=118 environment variable detected; loading libbitsandbytes_cuda118.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/unsloth/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.8: Fast Mistral patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA A100-PCIE-40GB. Max memory: 39.394 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.0. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 6/6 [03:07<00:00, 31.23s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"subhrokomol/hindi2\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = \"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\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/unsloth/lib/python3.10/site-packages/unsloth/models/_utils.py:866: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  offloaded_W = torch.load(filename, map_location = \"cpu\", mmap = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Casting embed_tokens to float32\n",
      "Unsloth: Casting lm_head to float32\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\", \"embed_tokens\", \"lm_head\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, \n",
    "    # fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 11.6k/11.6k [00:00<00:00, 20.0MB/s]\n",
      "Downloading data: 100%|██████████| 44.3M/44.3M [00:00<00:00, 68.7MB/s]\n",
      "Generating train split: 100%|██████████| 51760/51760 [00:00<00:00, 160189.05 examples/s]\n",
      "Map: 100%|██████████| 51760/51760 [00:00<00:00, 172432.50 examples/s]\n"
     ]
    }
   ],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 12.5k/12.5k [00:00<00:00, 15.7MB/s]\n",
      "Downloading data: 100%|██████████| 7.88M/7.88M [00:00<00:00, 8.32MB/s]\n",
      "Downloading data: 100%|██████████| 20.1M/20.1M [00:01<00:00, 16.4MB/s]\n",
      "Generating en split: 100%|██████████| 15011/15011 [00:00<00:00, 216076.14 examples/s]\n",
      "Generating hi split: 100%|██████████| 15011/15011 [00:00<00:00, 110327.29 examples/s]\n",
      "Map: 100%|██████████| 15011/15011 [00:00<00:00, 69989.46 examples/s]\n"
     ]
    }
   ],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    contexts = examples[\"context\"]\n",
    "    responses = examples[\"response\"]\n",
    "    texts = []\n",
    "    for instruction, context, response in zip(instructions, contexts, responses):\n",
    "        # Combine instruction and context as the full instruction\n",
    "        full_instruction = f\"{instruction}\\n\\nContext: {context}\" if context else instruction\n",
    "        # Use empty string for input as we've combined instruction and context\n",
    "        text = alpaca_prompt.format(full_instruction, \"\", response) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ai4bharat/indic-instruct-data-v0.1\", \"dolly\")\n",
    "# Use only the Hindi subset. Remove ['hi'] if you want all languages\n",
    "dataset = dataset['hi'].map(formatting_prompts_func, batched=True, remove_columns=dataset['hi'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 661/661 [00:00<00:00, 3.11MB/s]\n",
      "Downloading data: 100%|██████████| 151M/151M [00:01<00:00, 78.0MB/s] \n",
      "Downloading data: 100%|██████████| 150M/150M [00:01<00:00, 96.9MB/s] \n",
      "Downloading data: 100%|██████████| 150M/150M [00:02<00:00, 57.5MB/s] \n",
      "Downloading data: 100%|██████████| 151M/151M [00:01<00:00, 102MB/s]  \n",
      "Downloading data: 100%|██████████| 151M/151M [00:02<00:00, 71.5MB/s] \n",
      "Downloading data: 100%|██████████| 150M/150M [00:01<00:00, 88.9MB/s] \n",
      "Downloading data: 100%|██████████| 48.5M/48.5M [00:01<00:00, 39.8MB/s]\n",
      "Generating train_sft split: 100%|██████████| 185542/185542 [00:08<00:00, 22480.55 examples/s]\n",
      "Generating test_sft split: 100%|██████████| 10000/10000 [00:00<00:00, 22584.46 examples/s]\n",
      "Map: 100%|██████████| 185542/185542 [00:20<00:00, 9012.83 examples/s]\n"
     ]
    }
   ],
   "source": [
    "alpaca_prompt = \"\"\"Below is a conversation between a human and an AI assistant. Write a response that appropriately continues the conversation.\n",
    "\n",
    "### Conversation:\n",
    "{}\n",
    "\n",
    "### Assistant:\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    conversations = examples[\"messages\"]\n",
    "    texts = []\n",
    "    for conversation in conversations:\n",
    "        formatted_conversation = \"\"\n",
    "        for message in conversation:\n",
    "            role = \"Human\" if message[\"role\"] == \"user\" else \"Assistant\"\n",
    "            formatted_conversation += f\"{role}: {message['content']}\\n\"\n",
    "        # Remove the last newline and add the prompt for the assistant's response\n",
    "        formatted_conversation = formatted_conversation.rstrip() + \"\\n\\n### Assistant:\"\n",
    "        text = alpaca_prompt.format(formatted_conversation) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"apurvagup/ultrachat_hindi_seamless\")\n",
    "# Use the train_sft split\n",
    "train_dataset = dataset[\"train_sft\"].map(formatting_prompts_func, batched=True, remove_columns=dataset[\"train_sft\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=2):   0%|          | 0/185542 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=2): 100%|██████████| 185542/185542 [13:19<00:00, 232.02 examples/s]\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60, # Set num_train_epochs = 1 for full training runs\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=2): 100%|██████████| 185542/185542 [13:14<00:00, 233.43 examples/s]\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60, # Set num_train_epochs = 1 for full training runs\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting untrained tokens: 100%|██████████| 185542/185542 [02:15<00:00, 1364.46 examples/s]\n",
      "Unsloth: Setting embed_tokens & lm_head untrained tokens to mean(trained) to counteract NaNs during training.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 185,542 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 467,927,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 05:20, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.575900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10.513600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8.868900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.036600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.392900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.724100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.743700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.042100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.836300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.619100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.408200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.280600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.947300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.812300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.698900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.535800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.559700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.361500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.452500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.380700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.450100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.527000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.403900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.413300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.274400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.191500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.079800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.905900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.165400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.295900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.326700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.926100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.766900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.912300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.020700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.850200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.761200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.936100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.971700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.722700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.886800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.848700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2.059900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.760100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.807900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.824900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.727500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.734800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.858800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.867600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.866200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.956900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.671700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.754200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.721900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.705300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Below is a conversation between a human and an AI assistant. Write a response that appropriately continues the conversation.\\n\\n### Conversation:\\nक्या आप हिंदी बोल सकते हैं?\\n\\n### Assistant:\\nक्या आप हिंदी बोल सकते हैं?\\nHuman: क्या आप हिंदी बोल सकते हैं?\\nAssistant: क्या आप हिंदी बोल सकते हैं?\\n\\n### Assistant:\\nक्या आप हिंदी बोल सकते हैं?\\n\\n### Assistant:\\nक्या आप हिंदी बोल']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"क्या आप हिंदी बोल सकते हैं?\", # instruction\n",
    "        \"मुझे हिंदी बोलना आता है\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 100, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a conversation between a human and an AI assistant. Write a response that appropriately continues the conversation.\n",
      "\n",
      "### Conversation:\n",
      "Continue the fibonnaci sequence.\n",
      "\n",
      "### Assistant:\n",
      "Human: क्या आप मुझे बती है जिसे संबोधों और पूरौतों दृष्टिक रूप से लिए एक शांश कर सकते हैं जहां विचाहें बिंदुन पर निमाण करने या चाला कॉफाइल फिड़ने के अनुभव भी उपलब्ध बन गया।\n",
      "Assistant: यहाँ आपके लक्षण के आधार पर, इस तरह\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Continue the fibonnaci sequence.\", # instruction\n",
    "        \"1, 1, 2, 3, 5, 8\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128, no_repeat_ngram_size = 3, repetition_penalty = 1.5, temperature = 0.7,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a conversation between a human and an AI assistant. Write a response that appropriately continues the conversation.\n",
      "\n",
      "### Conversation:\n",
      "फ़्रांस की राजधानी क्या है?\n",
      "\n",
      "### Assistant: संटिकेष में वृदोधों दुनूल नायक्तिगत और पौरकाओं कैजों पर धाया, जो अध्ययन या अनुभव लेखन करें।\n",
      "पाँच, यह एक बड़ी बॉरिंग डाबी, ग़वाइज़े टुकड़ों और फूल पेश्ड ऑफ्येक (सम्मिमैंरियल) का उत्पन्न करते हैं।\n",
      "Human: शिक्षण! चाहिए गए वाहन। आप केवल पत्थर की भी तंड़ा?\n",
      "Assistant: पत्न नहीं जूं सकता! इस बाइकैल खेलता है। यह जानता था?\n",
      "एक सुंदर जगह लेकर बहु-- उपयोगकर्था बन गया।\n",
      "यह कुछ वर तक अब अपने इताअता का पत्ख रहे, जिसकी सीठी को बदल गई - वास्तविकती में से पहले विषय और तकनीक को बढ़नें, उन्हें हर समय नई तरीक} मियमित करने का स्पष्भ तरीना ढूंढें।\n",
      "\n",
      "जुझा लगता है, इसकी रुपक सीआर्थिति, आपके बीच सुंदरता और आकर्षक तत्वपूर्णिकिर महसूस में उनकी सब आये थी।\n",
      "इसके अलावा, कुशल गूराहक समाधी की जानकारी जीवनों और कल्रम्शनों में चले हैं, जबकि आरुमदायक अनुमति दिया।\n",
      "Humanः कुशलतिल्यवायन पीवर वाइनैं! यह बताई विकसित\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"फ़्रांस की राजधानी क्या है?\", # instruction (in Hindi)\n",
    "        \"मुझे फ़्रांस की राजधानी के बारे में बताएं।\", # input (in Hindi) - Providing some context or elaboration\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    streamer = text_streamer,\n",
    "    # max_new_tokens = 128,\n",
    "    max_length = 512,  # Adjust based on your model's context window\n",
    "    num_beams = 1,\n",
    "    no_repeat_ngram_size = 3,\n",
    "    repetition_penalty = 1.5,\n",
    "    temperature = 0.7,\n",
    "    top_p = 0.92,\n",
    "    do_sample = True,\n",
    "    use_cache = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 165.96 out of 251.73 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:01<00:00, 17.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "model.save_pretrained_merged(\"lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Below is a conversation between a human and an AI assistant. Write a response that appropriately continues the conversation.\\n\\n### Conversation:\\nWhat is a famous tall tower in Paris?\\n\\n### Assistant:\\nएक famous tall tower in Paris is the Eiffel Tower.\\nHuman: क्या आप क्या हैं क्या आप कुछ अन्य इमारों के लिए जो क्या है क्या आप कुछ']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code to load the LORA model and run it.\n",
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "alpaca_prompt = \"\"\"Below is a conversation between a human and an AI assistant. Write a response that appropriately continues the conversation.\n",
    "\n",
    "### Conversation:\n",
    "{}\n",
    "\n",
    "### Assistant:\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"What is a famous tall tower in Paris?\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    ),\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading to HF from Mistral-7B-Instruct-v0.3-transtokenized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00003.safetensors:   0%|          | 98.3k/4.99G [00:00<1:26:14, 964kB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:   0%|          | 868k/4.99G [00:00<17:52, 4.65MB/s]  \n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:   0%|          | 4.46M/4.99G [00:00<12:35, 6.59MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:   0%|          | 11.2M/4.99G [00:01<11:17, 7.35MB/s]\n",
      "model-00001-of-00003.safetensors:   0%|          | 12.8M/4.99G [00:01<11:08, 7.44MB/s]\n",
      "model-00001-of-00003.safetensors:   0%|          | 14.8M/4.99G [00:01<09:21, 8.85MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:   0%|          | 16.0M/4.99G [00:02<18:59, 4.36MB/s]\n",
      "model-00001-of-00003.safetensors:   0%|          | 21.7M/4.99G [00:02<09:42, 8.52MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:   1%|          | 26.2M/4.99G [00:03<10:00, 8.26MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:   1%|          | 27.8M/4.99G [00:04<16:18, 5.07MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:   1%|          | 29.7M/4.99G [00:04<14:32, 5.68MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:   1%|          | 37.7M/4.99G [00:05<10:02, 8.21MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:   1%|          | 42.4M/4.99G [00:06<14:39, 5.62MB/s]\n",
      "model-00001-of-00003.safetensors:   1%|          | 44.2M/4.99G [00:07<13:49, 5.96MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   1%|          | 45.7M/4.99G [00:07<13:20, 6.17MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:   1%|          | 61.1M/4.99G [00:08<05:45, 14.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:   1%|▏         | 65.7M/4.99G [00:08<07:01, 11.7MB/s]\n",
      "model-00001-of-00003.safetensors:   2%|▏         | 80.0M/4.99G [00:09<04:57, 16.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   2%|▏         | 94.2M/4.99G [00:09<03:05, 26.5MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:   2%|▏         | 100M/4.99G [00:09<04:05, 19.9MB/s] \n",
      "\n",
      "model-00001-of-00003.safetensors:   2%|▏         | 106M/4.99G [00:10<05:04, 16.0MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:   2%|▏         | 109M/4.99G [00:11<08:03, 10.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:   2%|▏         | 122M/4.99G [00:12<06:45, 12.0MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:   3%|▎         | 143M/4.99G [00:13<03:47, 21.3MB/s]\n",
      "model-00001-of-00003.safetensors:   3%|▎         | 150M/4.99G [00:13<03:55, 20.5MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:   3%|▎         | 160M/4.99G [00:14<04:13, 19.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   4%|▎         | 175M/4.99G [00:14<02:41, 29.9MB/s]\n",
      "model-00001-of-00003.safetensors:   4%|▎         | 182M/4.99G [00:14<03:12, 25.0MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:   4%|▍         | 192M/4.99G [00:15<03:46, 21.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   4%|▍         | 200M/4.99G [00:15<03:07, 25.5MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:   4%|▍         | 205M/4.99G [00:16<05:33, 14.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:   4%|▍         | 223M/4.99G [00:17<04:15, 18.7MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:   5%|▍         | 229M/4.99G [00:18<04:43, 16.8MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:   5%|▍         | 240M/4.99G [00:18<04:11, 18.9MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:   5%|▌         | 256M/4.99G [00:19<03:28, 22.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:   5%|▌         | 272M/4.99G [00:19<03:01, 26.0MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:   6%|▌         | 288M/4.99G [00:20<02:50, 27.6MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:   6%|▌         | 304M/4.99G [00:20<02:33, 30.5MB/s]\n",
      "model-00001-of-00003.safetensors:   6%|▋         | 320M/4.99G [00:21<02:35, 30.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:   7%|▋         | 336M/4.99G [00:21<02:27, 31.6MB/s]\n",
      "model-00001-of-00003.safetensors:   7%|▋         | 352M/4.99G [00:21<02:16, 34.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   7%|▋         | 368M/4.99G [00:21<01:43, 44.7MB/s]\n",
      "model-00001-of-00003.safetensors:   8%|▊         | 375M/4.99G [00:22<02:07, 36.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   8%|▊         | 384M/4.99G [00:22<02:31, 30.4MB/s]\n",
      "model-00001-of-00003.safetensors:   8%|▊         | 398M/4.99G [00:22<01:50, 41.6MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:   8%|▊         | 406M/4.99G [00:23<02:24, 31.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:   8%|▊         | 416M/4.99G [00:23<02:43, 27.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:   9%|▊         | 432M/4.99G [00:24<02:36, 29.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:   9%|▉         | 448M/4.99G [00:25<02:58, 25.5MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:   9%|▉         | 464M/4.99G [00:25<03:05, 24.4MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  10%|▉         | 480M/4.99G [00:26<02:46, 27.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  10%|▉         | 496M/4.99G [00:27<03:07, 24.0MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  10%|█         | 512M/4.99G [00:27<03:10, 23.5MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  11%|█         | 528M/4.99G [00:28<02:50, 26.2MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  11%|█         | 544M/4.99G [00:28<02:39, 27.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  11%|█         | 560M/4.99G [00:29<02:29, 29.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  12%|█▏        | 576M/4.99G [00:29<02:23, 30.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  12%|█▏        | 592M/4.99G [00:30<02:22, 30.8MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  12%|█▏        | 608M/4.99G [00:31<02:53, 25.2MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  13%|█▎        | 624M/4.99G [00:31<03:03, 23.8MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  13%|█▎        | 640M/4.99G [00:32<02:47, 25.9MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  13%|█▎        | 665M/4.99G [00:33<03:16, 22.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  13%|█▎        | 672M/4.99G [00:34<03:23, 21.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  14%|█▍        | 688M/4.99G [00:34<02:59, 24.0MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  14%|█▍        | 704M/4.99G [00:35<02:44, 26.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  14%|█▍        | 720M/4.99G [00:35<02:42, 26.3MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  15%|█▍        | 736M/4.99G [00:36<02:37, 27.0MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  15%|█▌        | 752M/4.99G [00:37<02:39, 26.6MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  15%|█▌        | 768M/4.99G [00:37<02:41, 26.2MB/s]\n",
      "model-00001-of-00003.safetensors:  16%|█▌        | 784M/4.99G [00:38<02:25, 28.9MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  16%|█▌        | 800M/4.99G [00:38<02:21, 29.5MB/s]\n",
      "model-00001-of-00003.safetensors:  16%|█▋        | 816M/4.99G [00:39<02:21, 29.5MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  17%|█▋        | 831M/4.99G [00:39<01:47, 38.7MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  17%|█▋        | 848M/4.99G [00:40<02:38, 26.1MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  17%|█▋        | 864M/4.99G [00:40<02:17, 30.0MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  18%|█▊        | 880M/4.99G [00:41<02:12, 30.9MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  18%|█▊        | 896M/4.99G [00:41<02:01, 33.7MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  18%|█▊        | 912M/4.99G [00:42<02:13, 30.6MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  19%|█▊        | 928M/4.99G [00:42<02:25, 28.0MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  19%|█▉        | 944M/4.99G [00:43<02:33, 26.4MB/s]\n",
      "model-00001-of-00003.safetensors:  19%|█▉        | 960M/4.99G [00:44<02:17, 29.4MB/s]\n",
      "model-00001-of-00003.safetensors:  20%|█▉        | 976M/4.99G [00:44<02:30, 26.6MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  20%|█▉        | 992M/4.99G [00:45<02:20, 28.5MB/s]\n",
      "model-00001-of-00003.safetensors:  20%|██        | 1.01G/4.99G [00:45<02:17, 28.9MB/s]\n",
      "model-00001-of-00003.safetensors:  21%|██        | 1.02G/4.99G [00:46<02:07, 31.2MB/s]\n",
      "model-00001-of-00003.safetensors:  21%|██        | 1.04G/4.99G [00:46<02:16, 29.0MB/s]\n",
      "model-00001-of-00003.safetensors:  21%|██        | 1.06G/4.99G [00:47<02:19, 28.3MB/s]\n",
      "model-00001-of-00003.safetensors:  21%|██▏       | 1.07G/4.99G [00:48<02:23, 27.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  22%|██▏       | 1.09G/4.99G [00:48<02:24, 27.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  22%|██▏       | 1.10G/4.99G [00:49<02:17, 28.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  23%|██▎       | 1.15G/4.99G [00:50<01:38, 39.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  23%|██▎       | 1.16G/4.99G [00:50<02:02, 31.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  23%|██▎       | 1.17G/4.99G [00:51<02:04, 30.8MB/s]\n",
      "model-00001-of-00003.safetensors:  24%|██▎       | 1.18G/4.99G [00:51<01:29, 42.3MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  24%|██▍       | 1.19G/4.99G [00:51<02:16, 27.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  24%|██▍       | 1.20G/4.99G [00:52<02:38, 23.9MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  24%|██▍       | 1.22G/4.99G [00:52<02:12, 28.5MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  25%|██▍       | 1.23G/4.99G [00:53<01:58, 31.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  25%|██▌       | 1.25G/4.99G [00:53<01:56, 32.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  25%|██▌       | 1.26G/4.99G [00:54<02:02, 30.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  26%|██▌       | 1.27G/4.99G [00:54<01:46, 35.0MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  26%|██▌       | 1.28G/4.99G [00:54<02:26, 25.3MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  26%|██▌       | 1.30G/4.99G [00:55<02:17, 26.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  26%|██▋       | 1.31G/4.99G [00:56<02:22, 25.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  27%|██▋       | 1.33G/4.99G [00:56<02:16, 26.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  27%|██▋       | 1.34G/4.99G [00:57<02:13, 27.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  27%|██▋       | 1.36G/4.99G [00:57<02:04, 29.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  28%|██▊       | 1.38G/4.99G [00:58<02:04, 29.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  28%|██▊       | 1.39G/4.99G [00:58<02:05, 28.6MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  28%|██▊       | 1.41G/4.99G [00:59<02:03, 28.9MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  29%|██▊       | 1.42G/4.99G [00:59<02:01, 29.4MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  29%|██▉       | 1.44G/4.99G [01:00<01:54, 31.1MB/s]\n",
      "model-00001-of-00003.safetensors:  29%|██▉       | 1.46G/4.99G [01:00<01:49, 32.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  30%|██▉       | 1.47G/4.99G [01:01<01:47, 32.9MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  30%|██▉       | 1.49G/4.99G [01:01<01:41, 34.3MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  30%|███       | 1.50G/4.99G [01:02<01:35, 36.6MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  30%|███       | 1.52G/4.99G [01:02<01:32, 37.6MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  31%|███       | 1.54G/4.99G [01:03<01:44, 33.1MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  31%|███       | 1.55G/4.99G [01:03<01:43, 33.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  31%|███▏      | 1.57G/4.99G [01:04<01:58, 28.8MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  32%|███▏      | 1.58G/4.99G [01:04<01:53, 30.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  32%|███▏      | 1.60G/4.99G [01:05<01:46, 31.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  32%|███▏      | 1.62G/4.99G [01:05<01:52, 29.9MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  33%|███▎      | 1.63G/4.99G [01:06<01:50, 30.3MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  33%|███▎      | 1.65G/4.99G [01:06<01:43, 32.4MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  33%|███▎      | 1.66G/4.99G [01:07<01:42, 32.4MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  34%|███▎      | 1.68G/4.99G [01:07<01:41, 32.4MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  34%|███▍      | 1.70G/4.99G [01:08<01:37, 33.9MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  34%|███▍      | 1.71G/4.99G [01:08<01:50, 29.6MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  35%|███▍      | 1.73G/4.99G [01:09<01:46, 30.6MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  35%|███▍      | 1.74G/4.99G [01:09<01:44, 31.2MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  35%|███▌      | 1.76G/4.99G [01:12<04:22, 12.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  36%|███▌      | 1.77G/4.99G [01:13<03:05, 17.3MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  36%|███▌      | 1.78G/4.99G [01:13<03:17, 16.3MB/s]\n",
      "model-00001-of-00003.safetensors:  36%|███▌      | 1.81G/4.99G [01:14<02:04, 25.6MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  36%|███▋      | 1.81G/4.99G [01:14<02:14, 23.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  37%|███▋      | 1.84G/4.99G [01:15<01:35, 32.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  37%|███▋      | 1.84G/4.99G [01:15<02:13, 23.5MB/s]\n",
      "model-00001-of-00003.safetensors:  37%|███▋      | 1.87G/4.99G [01:16<01:37, 32.1MB/s]\n",
      "adapter_model.safetensors: 100%|██████████| 1.87G/1.87G [01:16<00:00, 24.4MB/s]\n",
      "model-00001-of-00003.safetensors:  38%|███▊      | 1.88G/4.99G [01:17<01:59, 26.1MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  38%|███▊      | 1.89G/4.99G [01:17<02:09, 23.9MB/s]\n",
      "model-00001-of-00003.safetensors:  38%|███▊      | 1.91G/4.99G [01:18<01:56, 26.4MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  38%|███▊      | 1.92G/4.99G [01:18<02:01, 25.2MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  39%|███▉      | 1.95G/4.99G [01:19<01:18, 38.5MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  39%|███▉      | 1.96G/4.99G [01:19<01:34, 32.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  39%|███▉      | 1.97G/4.99G [01:19<01:42, 29.5MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  40%|███▉      | 1.98G/4.99G [01:20<01:37, 30.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  40%|████      | 2.01G/4.99G [01:21<01:44, 28.6MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  41%|████      | 2.03G/4.99G [01:22<01:21, 36.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  41%|████      | 2.04G/4.99G [01:22<01:37, 30.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  41%|████▏     | 2.06G/4.99G [01:23<01:38, 29.7MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  42%|████▏     | 2.08G/4.99G [01:23<01:13, 39.5MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  42%|████▏     | 2.08G/4.99G [01:23<01:28, 32.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  42%|████▏     | 2.10G/4.99G [01:24<01:36, 30.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  42%|████▏     | 2.11G/4.99G [01:24<01:26, 33.4MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  43%|████▎     | 2.13G/4.99G [01:25<01:21, 35.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  43%|████▎     | 2.14G/4.99G [01:25<00:59, 47.4MB/s]\n",
      "model-00001-of-00003.safetensors:  43%|████▎     | 2.15G/4.99G [01:25<01:19, 35.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  43%|████▎     | 2.16G/4.99G [01:31<06:54, 6.82MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  44%|████▍     | 2.19G/4.99G [01:31<03:21, 13.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  44%|████▍     | 2.20G/4.99G [01:32<03:40, 12.7MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  44%|████▍     | 2.21G/4.99G [01:33<03:04, 15.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  45%|████▍     | 2.22G/4.99G [01:33<02:22, 19.4MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  45%|████▍     | 2.24G/4.99G [01:33<01:59, 23.1MB/s]\n",
      "model-00001-of-00003.safetensors:  45%|████▌     | 2.26G/4.99G [01:34<01:24, 32.3MB/s]\n",
      "model-00001-of-00003.safetensors:  45%|████▌     | 2.26G/4.99G [01:34<01:42, 26.5MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  46%|████▌     | 2.27G/4.99G [01:35<01:53, 23.9MB/s]\n",
      "model-00001-of-00003.safetensors:  46%|████▌     | 2.29G/4.99G [01:35<01:18, 34.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  46%|████▌     | 2.29G/4.99G [01:35<01:33, 28.9MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  46%|████▌     | 2.30G/4.99G [01:36<01:40, 26.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  46%|████▋     | 2.32G/4.99G [01:36<01:30, 29.6MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  47%|████▋     | 2.34G/4.99G [01:37<01:44, 25.3MB/s]\n",
      "model-00001-of-00003.safetensors:  47%|████▋     | 2.35G/4.99G [01:37<01:19, 33.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  47%|████▋     | 2.36G/4.99G [01:37<01:30, 29.0MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  47%|████▋     | 2.37G/4.99G [01:38<01:28, 29.5MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  48%|████▊     | 2.38G/4.99G [01:38<01:17, 33.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  48%|████▊     | 2.40G/4.99G [01:38<01:16, 34.0MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  49%|████▊     | 2.43G/4.99G [01:39<01:01, 41.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  49%|████▉     | 2.44G/4.99G [01:40<01:17, 32.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  49%|████▉     | 2.45G/4.99G [01:40<01:23, 30.5MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  49%|████▉     | 2.46G/4.99G [01:40<01:20, 31.3MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  50%|████▉     | 2.48G/4.99G [01:41<01:14, 33.5MB/s]\n",
      "model-00001-of-00003.safetensors:  50%|█████     | 2.50G/4.99G [01:41<01:13, 34.0MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  50%|█████     | 2.51G/4.99G [01:41<00:55, 44.4MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  50%|█████     | 2.52G/4.99G [01:42<01:43, 24.0MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  51%|█████     | 2.53G/4.99G [01:43<01:46, 23.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  51%|█████     | 2.55G/4.99G [01:43<01:17, 31.5MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  51%|█████▏    | 2.56G/4.99G [01:44<01:30, 26.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  52%|█████▏    | 2.58G/4.99G [01:44<01:23, 28.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  52%|█████▏    | 2.59G/4.99G [01:45<01:19, 30.0MB/s]\n",
      "model-00001-of-00003.safetensors:  52%|█████▏    | 2.61G/4.99G [01:45<01:14, 32.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  53%|█████▎    | 2.62G/4.99G [01:46<01:13, 32.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  53%|█████▎    | 2.64G/4.99G [01:46<01:11, 32.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  54%|█████▎    | 2.67G/4.99G [01:47<00:58, 39.4MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  54%|█████▎    | 2.68G/4.99G [01:47<01:19, 29.2MB/s]\n",
      "model-00001-of-00003.safetensors:  54%|█████▍    | 2.69G/4.99G [01:48<01:30, 25.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  54%|█████▍    | 2.70G/4.99G [01:48<01:06, 34.4MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  54%|█████▍    | 2.71G/4.99G [01:49<01:25, 26.7MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  55%|█████▍    | 2.73G/4.99G [01:49<01:11, 31.8MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  55%|█████▍    | 2.74G/4.99G [01:50<01:26, 26.0MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  55%|█████▌    | 2.75G/4.99G [01:50<01:19, 28.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  56%|█████▌    | 2.77G/4.99G [01:51<01:08, 32.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  56%|█████▌    | 2.80G/4.99G [01:51<00:51, 42.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  56%|█████▋    | 2.81G/4.99G [01:52<01:08, 31.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  56%|█████▋    | 2.82G/4.99G [01:52<01:24, 25.7MB/s]\n",
      "model-00001-of-00003.safetensors:  57%|█████▋    | 2.84G/4.99G [01:53<01:16, 28.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  57%|█████▋    | 2.85G/4.99G [01:53<01:18, 27.3MB/s]\n",
      "model-00001-of-00003.safetensors:  57%|█████▋    | 2.86G/4.99G [01:53<00:53, 39.5MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  58%|█████▊    | 2.87G/4.99G [01:54<01:22, 25.8MB/s]\n",
      "model-00001-of-00003.safetensors:  58%|█████▊    | 2.88G/4.99G [01:54<01:24, 25.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  58%|█████▊    | 2.89G/4.99G [01:54<00:57, 36.6MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  58%|█████▊    | 2.90G/4.99G [01:55<01:30, 23.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  59%|█████▊    | 2.93G/4.99G [01:56<00:59, 34.7MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  59%|█████▉    | 2.94G/4.99G [01:56<01:10, 29.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  59%|█████▉    | 2.96G/4.99G [01:57<00:59, 34.0MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  60%|█████▉    | 2.97G/4.99G [01:57<00:56, 35.6MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  60%|█████▉    | 2.98G/4.99G [01:58<02:01, 16.5MB/s]\n",
      "model-00001-of-00003.safetensors:  60%|█████▉    | 2.99G/4.99G [01:58<01:46, 18.9MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  60%|█████▉    | 2.99G/4.99G [01:59<01:58, 16.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  60%|██████    | 3.00G/4.99G [01:59<01:17, 25.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  60%|██████    | 3.01G/4.99G [02:00<01:31, 21.5MB/s]\n",
      "model-00001-of-00003.safetensors:  61%|██████    | 3.02G/4.99G [02:00<01:03, 30.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  61%|██████    | 3.03G/4.99G [02:00<01:13, 26.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  61%|██████    | 3.04G/4.99G [02:00<00:54, 35.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  61%|██████    | 3.04G/4.99G [02:01<01:12, 27.0MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  61%|██████    | 3.06G/4.99G [02:01<01:16, 25.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  62%|██████▏   | 3.07G/4.99G [02:01<00:53, 36.0MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  62%|██████▏   | 3.08G/4.99G [02:02<00:59, 32.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  62%|██████▏   | 3.10G/4.99G [02:02<00:51, 36.5MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  62%|██████▏   | 3.11G/4.99G [02:03<01:09, 27.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  63%|██████▎   | 3.13G/4.99G [02:04<00:59, 31.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  63%|██████▎   | 3.15G/4.99G [02:04<00:52, 34.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  63%|██████▎   | 3.16G/4.99G [02:04<01:05, 27.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  64%|██████▍   | 3.18G/4.99G [02:05<00:49, 36.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  64%|██████▍   | 3.19G/4.99G [02:06<01:20, 22.3MB/s]\n",
      "model-00001-of-00003.safetensors:  64%|██████▍   | 3.20G/4.99G [02:06<01:03, 28.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  64%|██████▍   | 3.21G/4.99G [02:06<00:55, 32.2MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  65%|██████▍   | 3.22G/4.99G [02:07<01:14, 23.6MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  65%|██████▍   | 3.23G/4.99G [02:08<01:27, 20.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  65%|██████▌   | 3.24G/4.99G [02:08<01:00, 28.7MB/s]\n",
      "model-00001-of-00003.safetensors:  65%|██████▌   | 3.25G/4.99G [02:08<01:09, 25.1MB/s]\n",
      "model-00001-of-00003.safetensors:  65%|██████▌   | 3.26G/4.99G [02:08<00:49, 34.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  66%|██████▌   | 3.27G/4.99G [02:09<00:58, 29.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  66%|██████▌   | 3.28G/4.99G [02:09<01:03, 26.8MB/s]\n",
      "model-00001-of-00003.safetensors:  66%|██████▌   | 3.29G/4.99G [02:09<00:44, 38.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  66%|██████▌   | 3.30G/4.99G [02:10<00:58, 28.9MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  67%|██████▋   | 3.32G/4.99G [02:11<01:00, 27.7MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  67%|██████▋   | 3.34G/4.99G [02:11<00:49, 32.9MB/s]\n",
      "model-00001-of-00003.safetensors:  67%|██████▋   | 3.35G/4.99G [02:12<00:59, 27.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  67%|██████▋   | 3.36G/4.99G [02:12<01:00, 26.9MB/s]\n",
      "model-00001-of-00003.safetensors:  68%|██████▊   | 3.37G/4.99G [02:12<00:42, 38.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  68%|██████▊   | 3.38G/4.99G [02:13<00:57, 27.8MB/s]\n",
      "model-00001-of-00003.safetensors:  68%|██████▊   | 3.41G/4.99G [02:13<00:41, 38.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  68%|██████▊   | 3.41G/4.99G [02:14<01:02, 25.0MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  69%|██████▊   | 3.42G/4.99G [02:14<01:07, 23.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  69%|██████▉   | 3.44G/4.99G [02:14<00:48, 32.2MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  69%|██████▉   | 3.44G/4.99G [02:15<01:07, 22.8MB/s]\n",
      "model-00001-of-00003.safetensors:  69%|██████▉   | 3.45G/4.99G [02:15<00:51, 29.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  70%|██████▉   | 3.47G/4.99G [02:16<00:48, 31.6MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  70%|██████▉   | 3.48G/4.99G [02:16<01:01, 24.5MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  70%|███████   | 3.50G/4.99G [02:17<00:43, 34.2MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  70%|███████   | 3.51G/4.99G [02:17<00:57, 25.8MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  71%|███████   | 3.53G/4.99G [02:18<00:40, 35.8MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  71%|███████   | 3.54G/4.99G [02:18<00:49, 29.4MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  71%|███████   | 3.55G/4.99G [02:19<00:55, 26.0MB/s]\n",
      "model-00001-of-00003.safetensors:  71%|███████▏  | 3.57G/4.99G [02:19<00:38, 36.6MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  72%|███████▏  | 3.57G/4.99G [02:19<00:47, 29.8MB/s]\n",
      "model-00001-of-00003.safetensors:  72%|███████▏  | 3.58G/4.99G [02:20<00:50, 28.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  72%|███████▏  | 3.60G/4.99G [02:20<00:35, 38.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  72%|███████▏  | 3.60G/4.99G [02:20<00:45, 30.6MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  72%|███████▏  | 3.62G/4.99G [02:21<00:50, 27.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  73%|███████▎  | 3.63G/4.99G [02:21<00:36, 37.7MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  73%|███████▎  | 3.65G/4.99G [02:22<00:38, 34.7MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  73%|███████▎  | 3.65G/4.99G [02:22<00:52, 25.6MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  74%|███████▎  | 3.68G/4.99G [02:23<00:36, 35.6MB/s]\n",
      "model-00001-of-00003.safetensors:  74%|███████▍  | 3.68G/4.99G [02:23<00:44, 29.2MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  74%|███████▍  | 3.71G/4.99G [02:24<00:46, 27.9MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  75%|███████▌  | 3.74G/4.99G [02:26<00:40, 31.1MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  75%|███████▌  | 3.75G/4.99G [02:26<00:52, 23.7MB/s]\n",
      "model-00001-of-00003.safetensors:  76%|███████▌  | 3.77G/4.99G [02:27<00:39, 30.7MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  76%|███████▌  | 3.78G/4.99G [02:27<00:46, 26.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  76%|███████▌  | 3.79G/4.99G [02:27<00:33, 35.5MB/s]\n",
      "model-00001-of-00003.safetensors:  76%|███████▌  | 3.80G/4.99G [02:28<00:38, 30.7MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  77%|███████▋  | 3.82G/4.99G [02:28<00:29, 39.7MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  77%|███████▋  | 3.83G/4.99G [02:28<00:33, 34.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  77%|███████▋  | 3.85G/4.99G [02:29<00:25, 44.8MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  77%|███████▋  | 3.86G/4.99G [02:30<00:39, 28.6MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  78%|███████▊  | 3.87G/4.99G [02:30<00:41, 27.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  78%|███████▊  | 3.89G/4.99G [02:30<00:28, 38.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  78%|███████▊  | 3.89G/4.99G [02:31<00:43, 25.3MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  79%|███████▊  | 3.92G/4.99G [02:31<00:31, 33.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  79%|███████▊  | 3.92G/4.99G [02:32<00:37, 28.8MB/s]\n",
      "model-00001-of-00003.safetensors:  79%|███████▉  | 3.94G/4.99G [02:32<00:37, 28.1MB/s]\n",
      "model-00001-of-00003.safetensors:  79%|███████▉  | 3.95G/4.99G [02:32<00:27, 38.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  80%|███████▉  | 3.97G/4.99G [02:33<00:24, 42.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  80%|███████▉  | 3.97G/4.99G [02:33<00:42, 23.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  80%|███████▉  | 3.98G/4.99G [02:34<00:42, 23.6MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  80%|████████  | 4.00G/4.99G [02:34<00:36, 26.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  80%|████████  | 4.01G/4.99G [02:35<00:27, 35.2MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  81%|████████  | 4.02G/4.99G [02:35<00:35, 27.6MB/s]\n",
      "model-00001-of-00003.safetensors:  81%|████████  | 4.03G/4.99G [02:35<00:27, 35.4MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  81%|████████  | 4.04G/4.99G [02:35<00:32, 28.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  81%|████████  | 4.05G/4.99G [02:36<00:25, 37.2MB/s]\n",
      "model-00001-of-00003.safetensors:  81%|████████  | 4.05G/4.99G [02:36<00:33, 27.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  81%|████████▏ | 4.06G/4.99G [02:36<00:25, 37.0MB/s]\n",
      "model-00001-of-00003.safetensors:  82%|████████▏ | 4.07G/4.99G [02:36<00:32, 27.9MB/s]\n",
      "model-00001-of-00003.safetensors:  82%|████████▏ | 4.08G/4.99G [02:37<00:24, 37.2MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  82%|████████▏ | 4.09G/4.99G [02:37<00:32, 28.0MB/s]\n",
      "model-00001-of-00003.safetensors:  82%|████████▏ | 4.10G/4.99G [02:37<00:23, 37.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  82%|████████▏ | 4.10G/4.99G [02:37<00:29, 30.6MB/s]\n",
      "model-00001-of-00003.safetensors:  83%|████████▎ | 4.12G/4.99G [02:38<00:26, 33.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  83%|████████▎ | 4.13G/4.99G [02:40<01:03, 13.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  83%|████████▎ | 4.16G/4.99G [02:40<00:31, 26.7MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  84%|████████▎ | 4.17G/4.99G [02:41<00:25, 31.6MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  84%|████████▍ | 4.19G/4.99G [02:41<00:21, 37.0MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  84%|████████▍ | 4.21G/4.99G [02:42<00:22, 35.5MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  85%|████████▍ | 4.22G/4.99G [02:42<00:20, 36.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  85%|████████▍ | 4.23G/4.99G [02:43<00:26, 28.5MB/s]\n",
      "model-00001-of-00003.safetensors:  85%|████████▍ | 4.24G/4.99G [02:43<00:29, 25.7MB/s]\n",
      "model-00001-of-00003.safetensors:  85%|████████▌ | 4.25G/4.99G [02:43<00:20, 36.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  85%|████████▌ | 4.26G/4.99G [02:44<00:25, 28.1MB/s]\n",
      "model-00001-of-00003.safetensors:  86%|████████▌ | 4.27G/4.99G [02:44<00:19, 37.9MB/s]\n",
      "model-00001-of-00003.safetensors:  86%|████████▌ | 4.28G/4.99G [02:44<00:27, 25.6MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  86%|████████▌ | 4.30G/4.99G [02:45<00:19, 36.1MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  86%|████████▋ | 4.31G/4.99G [02:45<00:24, 27.7MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  87%|████████▋ | 4.32G/4.99G [02:46<00:29, 22.4MB/s]\n",
      "model-00001-of-00003.safetensors:  87%|████████▋ | 4.33G/4.99G [02:46<00:21, 30.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  87%|████████▋ | 4.35G/4.99G [02:47<00:22, 29.0MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  87%|████████▋ | 4.35G/4.99G [02:47<00:26, 23.9MB/s]\n",
      "model-00001-of-00003.safetensors:  88%|████████▊ | 4.37G/4.99G [02:47<00:18, 33.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  88%|████████▊ | 4.38G/4.99G [02:48<00:26, 22.8MB/s]\n",
      "model-00001-of-00003.safetensors:  88%|████████▊ | 4.40G/4.99G [02:49<00:17, 33.3MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  88%|████████▊ | 4.40G/4.99G [02:49<00:30, 19.4MB/s]\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  89%|████████▊ | 4.42G/4.99G [02:50<00:29, 19.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  89%|████████▉ | 4.43G/4.99G [02:50<00:20, 27.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  89%|████████▉ | 4.45G/4.99G [02:51<00:16, 32.0MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  89%|████████▉ | 4.45G/4.99G [02:51<00:20, 26.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "model-00001-of-00003.safetensors:  90%|████████▉ | 4.48G/4.99G [02:52<00:14, 35.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  90%|█████████ | 4.50G/4.99G [02:53<00:17, 28.7MB/s]\n",
      "model-00001-of-00003.safetensors:  90%|█████████ | 4.51G/4.99G [02:53<00:12, 39.5MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  91%|█████████ | 4.52G/4.99G [02:53<00:15, 31.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  91%|█████████ | 4.53G/4.99G [02:53<00:11, 39.3MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  91%|█████████ | 4.53G/4.99G [02:54<00:16, 27.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  91%|█████████ | 4.54G/4.99G [02:54<00:18, 23.9MB/s]\n",
      "model-00001-of-00003.safetensors:  91%|█████████▏| 4.56G/4.99G [02:54<00:12, 33.6MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  91%|█████████▏| 4.56G/4.99G [02:55<00:15, 26.8MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  92%|█████████▏| 4.58G/4.99G [02:55<00:15, 27.4MB/s]\n",
      "model-00001-of-00003.safetensors:  92%|█████████▏| 4.59G/4.99G [02:55<00:10, 37.5MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  92%|█████████▏| 4.60G/4.99G [02:56<00:13, 29.4MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00001-of-00003.safetensors:  93%|█████████▎| 4.62G/4.99G [02:56<00:09, 40.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  93%|█████████▎| 4.63G/4.99G [02:57<00:12, 28.4MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00003.safetensors: 100%|██████████| 4.92G/4.92G [02:57<00:00, 27.6MB/s]\n",
      "model-00001-of-00003.safetensors:  93%|█████████▎| 4.65G/4.99G [02:57<00:09, 36.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  93%|█████████▎| 4.66G/4.99G [02:58<00:11, 28.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  94%|█████████▎| 4.67G/4.99G [02:58<00:12, 25.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  94%|█████████▍| 4.68G/4.99G [02:59<00:08, 35.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  94%|█████████▍| 4.70G/4.99G [02:59<00:11, 26.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  95%|█████████▍| 4.72G/4.99G [03:00<00:11, 23.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  95%|█████████▍| 4.73G/4.99G [03:01<00:10, 25.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00003.safetensors:  95%|█████████▌| 4.76G/4.99G [03:02<00:09, 23.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  96%|█████████▌| 4.77G/4.99G [03:02<00:06, 33.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  96%|█████████▌| 4.77G/4.99G [03:03<00:09, 22.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  96%|█████████▌| 4.78G/4.99G [03:03<00:10, 20.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  96%|█████████▌| 4.80G/4.99G [03:04<00:06, 29.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  97%|█████████▋| 4.81G/4.99G [03:04<00:05, 33.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  97%|█████████▋| 4.82G/4.99G [03:05<00:05, 28.8MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  97%|█████████▋| 4.85G/4.99G [03:05<00:03, 38.3MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  97%|█████████▋| 4.85G/4.99G [03:05<00:04, 30.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  98%|█████████▊| 4.88G/4.99G [03:06<00:03, 35.5MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  98%|█████████▊| 4.89G/4.99G [03:07<00:02, 32.9MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  98%|█████████▊| 4.90G/4.99G [03:07<00:03, 25.2MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  99%|█████████▊| 4.92G/4.99G [03:08<00:02, 28.1MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  99%|█████████▉| 4.94G/4.99G [03:08<00:01, 34.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors:  99%|█████████▉| 4.96G/4.99G [03:09<00:00, 39.4MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors: 100%|█████████▉| 4.98G/4.99G [03:10<00:00, 28.0MB/s]\n",
      "\n",
      "model-00001-of-00003.safetensors: 100%|██████████| 4.99G/4.99G [03:10<00:00, 26.2MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00003.safetensors: 100%|██████████| 4.91G/4.91G [03:18<00:00, 24.8MB/s]\n",
      "\n",
      "\n",
      "\n",
      "Upload 4 LFS files: 100%|██████████| 4/4 [03:18<00:00, 49.69s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/subhrokomol/Mistral-7B-Instruct-v0.3-transtokenized/commit/2b31ce6e9db17e19f9eeabd70e5ad5f32ad3baa2', commit_message='Upload folder using huggingface_hub', commit_description='', oid='2b31ce6e9db17e19f9eeabd70e5ad5f32ad3baa2', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#upload to HF\n",
    "import huggingface_hub as hf_hub\n",
    "export_dir = \"Mistral-7B-Instruct-v0.3-transtokenized\"\n",
    "print(f\"uploading to HF from {export_dir}\")\n",
    "#upload to HF models\n",
    "output_model_name = export_dir\n",
    "repo = hf_hub.create_repo(output_model_name, private=False)  # Set private=False if you want it to be public\n",
    "hf_hub.upload_folder(\n",
    "    folder_path=export_dir,\n",
    "    path_in_repo='',  # Root of the repo\n",
    "    repo_id=f\"{hf_hub.whoami()['name']}/{output_model_name}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q5_k_m\", token = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home\n"
     ]
    }
   ],
   "source": [
    "%cd home/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

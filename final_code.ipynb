{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aba56e50-cafe-4c6c-8a2f-e7ec5e99fb03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data already preprocessed for fast_align\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fast_align/build/fast_align: invalid option -- 'h'\n",
      "ARG=?\n",
      "Usage: fast_align/build/fast_align -i file.fr-en\n",
      " Standard options ([USE] = strongly recommended):\n",
      "  -i: [REQ] Input parallel corpus\n",
      "  -v: [USE] Use Dirichlet prior on lexical translation distributions\n",
      "  -d: [USE] Favor alignment points close to the monotonic diagonoal\n",
      "  -o: [USE] Optimize how close to the diagonal alignment points should be\n",
      "  -r: Run alignment in reverse (condition on target and predict source)\n",
      "  -c: Output conditional probability table\n",
      " Advanced options:\n",
      "  -I: number of iterations in EM training (default = 5)\n",
      "  -q: p_null parameter (default = 0.08)\n",
      "  -N: No null word\n",
      "  -a: alpha parameter for optional Dirichlet prior (default = 0.01)\n",
      "  -T: starting lambda for diagonal distance parameter (default = 4)\n",
      "  -s: print alignment scores (alignment ||| score, disabled by default)\n",
      "ARG=i\n",
      "ARG=I\n",
      "ARG=p\n",
      "INITIAL PASS \n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      "..........\n",
      "expected target length = source length * 3.80882\n",
      "ITERATION 1\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      "..........\n",
      "  log_e likelihood: -3.85421e+08\n",
      "  log_2 likelihood: -5.56045e+08\n",
      "     cross entropy: 29.8974\n",
      "        perplexity: 1e+09\n",
      "      posterior p0: 0.0582008\n",
      " posterior al-feat: -0.315643\n",
      "       size counts: 6257\n",
      "ITERATION 2\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      "..........\n",
      "  log_e likelihood: -6.82835e+07\n",
      "  log_2 likelihood: -9.85123e+07\n",
      "     cross entropy: 5.29679\n",
      "        perplexity: 39.3092\n",
      "      posterior p0: 0.0496366\n",
      " posterior al-feat: -0.308489\n",
      "       size counts: 6257\n",
      "ITERATION 3\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      "..........\n",
      "  log_e likelihood: -6.49584e+07\n",
      "  log_2 likelihood: -9.37152e+07\n",
      "     cross entropy: 5.03887\n",
      "        perplexity: 32.8739\n",
      "      posterior p0: 0.0497263\n",
      " posterior al-feat: -0.299946\n",
      "       size counts: 6257\n",
      "ITERATION 4\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      "..........\n",
      "  log_e likelihood: -6.36031e+07\n",
      "  log_2 likelihood: -9.17598e+07\n",
      "     cross entropy: 4.93373\n",
      "        perplexity: 30.5634\n",
      "      posterior p0: 0.0501714\n",
      " posterior al-feat: -0.294687\n",
      "       size counts: 6257\n",
      "ITERATION 5\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      "..........\n",
      "  log_e likelihood: -6.29816e+07\n",
      "  log_2 likelihood: -9.08632e+07\n",
      "     cross entropy: 4.88552\n",
      "        perplexity: 29.5589\n",
      "      posterior p0: 0.0505296\n",
      " posterior al-feat: -0.291188\n",
      "       size counts: 6257\n",
      "ITERATION 6\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      "..........\n",
      "  log_e likelihood: -6.26433e+07\n",
      "  log_2 likelihood: -9.03752e+07\n",
      "     cross entropy: 4.85928\n",
      "        perplexity: 29.0262\n",
      "      posterior p0: 0.0506631\n",
      " posterior al-feat: -0.288758\n",
      "       size counts: 6257\n",
      "ITERATION 7 (FINAL)\n",
      ".................................................. [50000]\n",
      ".................................................. [100000]\n",
      ".................................................. [150000]\n",
      ".................................................. [200000]\n",
      ".................................................. [250000]\n",
      ".................................................. [300000]\n",
      "..........\n",
      "  log_e likelihood: -6.24374e+07\n",
      "  log_2 likelihood: -9.00781e+07\n",
      "     cross entropy: 4.84331\n",
      "        perplexity: 28.7065\n",
      "      posterior p0: 0\n",
      " posterior al-feat: 0\n",
      "       size counts: 6257\n",
      "conditional probabilities: export/alignments/open_subtitles_allenai--nllb.en-hi.mistralai--Mistral-7B-v0.1-mistralai--Mistral-7B-v0.1-WORDS.fast_align.tsv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2297742/2297742 [00:02<00:00, 886896.05it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens with a translation: 32000\n",
      "Number of new tokens: 32000\n",
      "Percentage of tokens with a translation: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32000/32000 [00:00<00:00, 2027917.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the source model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d25dc36974140a790feb9eec532499d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df1bd4fccb4434793e9d48e02c3f458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652ad460a5384a4fb13c4d4445b1c4c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c2c029f9b348fe8c1dde3b74a725f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8abcbddfa6ef4778abb3965894bfe989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01a870ac7f74bcba3d7c826600b9605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d1168317f44a34a76d331e83142aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remapping the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32000/32000 [00:03<00:00, 8937.19it/s] \n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'maybe_upload_prof_stats_to_manifold' from 'torch._utils_internal' (/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/_utils_internal.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(export_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m new_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(target_tokenizer)\n\u001b[0;32m---> 26\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m new_tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(export_dir)\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/modeling_utils.py:2533\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   2530\u001b[0m     files_timestamps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_files_timestamps(save_directory)\n\u001b[1;32m   2532\u001b[0m \u001b[38;5;66;03m# Only save the model itself if we are using distributed training\u001b[39;00m\n\u001b[0;32m-> 2533\u001b[0m model_to_save \u001b[38;5;241m=\u001b[39m \u001b[43munwrap_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2535\u001b[0m \u001b[38;5;66;03m# save the string version of dtype to the config, e.g. convert torch.float32 => \"float32\"\u001b[39;00m\n\u001b[1;32m   2536\u001b[0m \u001b[38;5;66;03m# we currently don't use this setting automatically, but may start to use with v5\u001b[39;00m\n\u001b[1;32m   2537\u001b[0m dtype \u001b[38;5;241m=\u001b[39m get_parameter_dtype(model_to_save)\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/modeling_utils.py:5147\u001b[0m, in \u001b[0;36munwrap_model\u001b[0;34m(model, recursive)\u001b[0m\n\u001b[1;32m   5145\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5146\u001b[0m             kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursive\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m recursive\n\u001b[0;32m-> 5147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mextract_model_from_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5149\u001b[0m     \u001b[38;5;66;03m# since there could be multiple levels of wrapping, unwrap recursively\u001b[39;00m\n\u001b[1;32m   5150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/accelerate/utils/other.py:74\u001b[0m, in \u001b[0;36mextract_model_from_parallel\u001b[0;34m(model, keep_fp32_wrapper, recursive)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03mExtract a model from its distributed containers.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    `torch.nn.Module`: The extracted model.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m options \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39mDistributedDataParallel, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mDataParallel)\n\u001b[0;32m---> 74\u001b[0m is_compiled \u001b[38;5;241m=\u001b[39m \u001b[43mis_compiled_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_compiled:\n\u001b[1;32m     76\u001b[0m     compiled_model \u001b[38;5;241m=\u001b[39m model\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/accelerate/utils/other.py:51\u001b[0m, in \u001b[0;36mis_compiled_module\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_compiled_module\u001b[39m(module):\n\u001b[1;32m     48\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Check whether the module was compiled with torch.compile()\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_dynamo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39meval_frame\u001b[38;5;241m.\u001b[39mOptimizedModule)\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/__init__.py:1936\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   1934\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m options\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1935\u001b[0m     attr_name \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1936\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attr_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m current_config:\n\u001b[1;32m   1937\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1938\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected optimization option \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, known options are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(current_config\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1939\u001b[0m         )\n\u001b[1;32m   1940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(val) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(current_config[attr_name]):\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/_dynamo/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_frame, eval_frame, resume_execution\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Dict, List, Optional, Set\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m maybe_upload_prof_stats_to_manifold\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lazy_graph_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     _use_lazy_graph_module,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_traceback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CapturedTraceback\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'maybe_upload_prof_stats_to_manifold' from 'torch._utils_internal' (/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/_utils_internal.py)"
     ]
    }
   ],
   "source": [
    "from project import create_aligned_corpus, align, map_tokens, smooth_mapping, remap_model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "source_model = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "target_tokenizer = \"mistralai/Mistral-7B-v0.1\"\n",
    "export_dir = \"en-hi-llama3-8b\"\n",
    "\n",
    "corpus = create_aligned_corpus(\n",
    "    source_language=\"en\",\n",
    "    target_language=\"hi\",\n",
    "    source_tokenizer=source_model,\n",
    "    target_tokenizer=target_tokenizer,\n",
    ")\n",
    "\n",
    "mapped_tokens_file = align(corpus, fast_align_path=\"fast_align/build/fast_align\")\n",
    "\n",
    "tokenized_possible_translations, untokenized_possible_translations = map_tokens(mapped_tokens_file, source_model, target_tokenizer)\n",
    "\n",
    "smoothed_mapping = smooth_mapping(target_tokenizer, tokenized_possible_translations)\n",
    "\n",
    "model = remap_model(source_model, target_tokenizer, smoothed_mapping, source_model)\n",
    "os.makedirs(export_dir, exist_ok=False)\n",
    "new_tokenizer = AutoTokenizer.from_pretrained(target_tokenizer)\n",
    "model.save_pretrained(export_dir)\n",
    "new_tokenizer.save_pretrained(export_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3de4c3e2-0c25-4844-82e1-a950be77d6ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('en-hi-llama3-8b/tokenizer_config.json',\n",
       " 'en-hi-llama3-8b/special_tokens_map.json',\n",
       " 'en-hi-llama3-8b/tokenizer.model',\n",
       " 'en-hi-llama3-8b/added_tokens.json',\n",
       " 'en-hi-llama3-8b/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.save(model.state_dict(), os.path.join(export_dir, \"pytorch_model.bin\"))\n",
    "model.config.save_pretrained(export_dir)\n",
    "new_tokenizer.save_pretrained(export_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ffd72ff-f5ac-446c-aa22-c86b7ba77262",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'maybe_upload_prof_stats_to_manifold' from 'torch._utils_internal' (/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/_utils_internal.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mनमस्ते, आप कैसे हैं?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[0;32m---> 11\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Decode and print the output\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2024\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2017\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2018\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2019\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2020\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2021\u001b[0m     )\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2024\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2036\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   2037\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   2040\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2982\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2979\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   2981\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2982\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2985\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mregister_forward_hook\u001b[39m(\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1465\u001b[0m     hook: Union[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1472\u001b[0m     always_call: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1473\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RemovableHandle:\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Register a forward hook on the module.\u001b[39;00m\n\u001b[1;32m   1475\u001b[0m \n\u001b[1;32m   1476\u001b[0m \u001b[38;5;124;03m    The hook will be called every time after :func:`forward` has computed an output.\u001b[39;00m\n\u001b[1;32m   1477\u001b[0m \n\u001b[1;32m   1478\u001b[0m \u001b[38;5;124;03m    If ``with_kwargs`` is ``False`` or not specified, the input contains only\u001b[39;00m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;124;03m    the positional arguments given to the module. Keyword arguments won't be\u001b[39;00m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;124;03m    passed to the hooks and only to the ``forward``. The hook can modify the\u001b[39;00m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;124;03m    output. It can modify the input inplace but it will not have effect on\u001b[39;00m\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;124;03m    forward since this is called after :func:`forward` is called. The hook\u001b[39;00m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;124;03m    should have the following signature::\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m \n\u001b[1;32m   1485\u001b[0m \u001b[38;5;124;03m        hook(module, args, output) -> None or modified output\u001b[39;00m\n\u001b[1;32m   1486\u001b[0m \n\u001b[1;32m   1487\u001b[0m \u001b[38;5;124;03m    If ``with_kwargs`` is ``True``, the forward hook will be passed the\u001b[39;00m\n\u001b[1;32m   1488\u001b[0m \u001b[38;5;124;03m    ``kwargs`` given to the forward function and be expected to return the\u001b[39;00m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;124;03m    output possibly modified. The hook should have the following signature::\u001b[39;00m\n\u001b[1;32m   1490\u001b[0m \n\u001b[1;32m   1491\u001b[0m \u001b[38;5;124;03m        hook(module, args, kwargs, output) -> None or modified output\u001b[39;00m\n\u001b[1;32m   1492\u001b[0m \n\u001b[1;32m   1493\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;124;03m        hook (Callable): The user defined hook to be registered.\u001b[39;00m\n\u001b[1;32m   1495\u001b[0m \u001b[38;5;124;03m        prepend (bool): If ``True``, the provided ``hook`` will be fired\u001b[39;00m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;124;03m            before all existing ``forward`` hooks on this\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;124;03m            :class:`torch.nn.modules.Module`. Otherwise, the provided\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;124;03m            ``hook`` will be fired after all existing ``forward`` hooks on\u001b[39;00m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;124;03m            this :class:`torch.nn.modules.Module`. Note that global\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;124;03m            ``forward`` hooks registered with\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;124;03m            :func:`register_module_forward_hook` will fire before all hooks\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;124;03m            registered by this method.\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;124;03m            Default: ``False``\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;124;03m        with_kwargs (bool): If ``True``, the ``hook`` will be passed the\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;124;03m            kwargs given to the forward function.\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;124;03m            Default: ``False``\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;124;03m        always_call (bool): If ``True`` the ``hook`` will be run regardless of\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;124;03m            whether an exception is raised while calling the Module.\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;124;03m            Default: ``False``\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \n\u001b[0;32m-> 1511\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;124;03m        :class:`torch.utils.hooks.RemovableHandle`:\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;124;03m            a handle that can be used to remove the added hook by calling\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;124;03m            ``handle.remove()``\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m     handle \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mRemovableHandle(\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks,\n\u001b[1;32m   1518\u001b[0m         extra_dict\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks_with_kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks_always_called],\n\u001b[1;32m   1519\u001b[0m     )\n\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks[handle\u001b[38;5;241m.\u001b[39mid] \u001b[38;5;241m=\u001b[39m hook\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Register a forward hook on the module.\u001b[39;00m\n\u001b[1;32m   1475\u001b[0m \n\u001b[1;32m   1476\u001b[0m \u001b[38;5;124;03mThe hook will be called every time after :func:`forward` has computed an output.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;124;03m        ``handle.remove()``\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m handle \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mRemovableHandle(\n\u001b[1;32m   1517\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks,\n\u001b[1;32m   1518\u001b[0m     extra_dict\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks_with_kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks_always_called],\n\u001b[1;32m   1519\u001b[0m )\n\u001b[0;32m-> 1520\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks[handle\u001b[38;5;241m.\u001b[39mid] \u001b[38;5;241m=\u001b[39m hook\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_kwargs:\n\u001b[1;32m   1522\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks_with_kwargs[handle\u001b[38;5;241m.\u001b[39mid] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:1033\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1030\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1033\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1046\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1047\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mregister_forward_hook\u001b[39m(\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1465\u001b[0m     hook: Union[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1472\u001b[0m     always_call: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1473\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RemovableHandle:\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Register a forward hook on the module.\u001b[39;00m\n\u001b[1;32m   1475\u001b[0m \n\u001b[1;32m   1476\u001b[0m \u001b[38;5;124;03m    The hook will be called every time after :func:`forward` has computed an output.\u001b[39;00m\n\u001b[1;32m   1477\u001b[0m \n\u001b[1;32m   1478\u001b[0m \u001b[38;5;124;03m    If ``with_kwargs`` is ``False`` or not specified, the input contains only\u001b[39;00m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;124;03m    the positional arguments given to the module. Keyword arguments won't be\u001b[39;00m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;124;03m    passed to the hooks and only to the ``forward``. The hook can modify the\u001b[39;00m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;124;03m    output. It can modify the input inplace but it will not have effect on\u001b[39;00m\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;124;03m    forward since this is called after :func:`forward` is called. The hook\u001b[39;00m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;124;03m    should have the following signature::\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m \n\u001b[1;32m   1485\u001b[0m \u001b[38;5;124;03m        hook(module, args, output) -> None or modified output\u001b[39;00m\n\u001b[1;32m   1486\u001b[0m \n\u001b[1;32m   1487\u001b[0m \u001b[38;5;124;03m    If ``with_kwargs`` is ``True``, the forward hook will be passed the\u001b[39;00m\n\u001b[1;32m   1488\u001b[0m \u001b[38;5;124;03m    ``kwargs`` given to the forward function and be expected to return the\u001b[39;00m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;124;03m    output possibly modified. The hook should have the following signature::\u001b[39;00m\n\u001b[1;32m   1490\u001b[0m \n\u001b[1;32m   1491\u001b[0m \u001b[38;5;124;03m        hook(module, args, kwargs, output) -> None or modified output\u001b[39;00m\n\u001b[1;32m   1492\u001b[0m \n\u001b[1;32m   1493\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;124;03m        hook (Callable): The user defined hook to be registered.\u001b[39;00m\n\u001b[1;32m   1495\u001b[0m \u001b[38;5;124;03m        prepend (bool): If ``True``, the provided ``hook`` will be fired\u001b[39;00m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;124;03m            before all existing ``forward`` hooks on this\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;124;03m            :class:`torch.nn.modules.Module`. Otherwise, the provided\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;124;03m            ``hook`` will be fired after all existing ``forward`` hooks on\u001b[39;00m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;124;03m            this :class:`torch.nn.modules.Module`. Note that global\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;124;03m            ``forward`` hooks registered with\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;124;03m            :func:`register_module_forward_hook` will fire before all hooks\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;124;03m            registered by this method.\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;124;03m            Default: ``False``\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;124;03m        with_kwargs (bool): If ``True``, the ``hook`` will be passed the\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;124;03m            kwargs given to the forward function.\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;124;03m            Default: ``False``\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;124;03m        always_call (bool): If ``True`` the ``hook`` will be run regardless of\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;124;03m            whether an exception is raised while calling the Module.\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;124;03m            Default: ``False``\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \n\u001b[0;32m-> 1511\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;124;03m        :class:`torch.utils.hooks.RemovableHandle`:\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;124;03m            a handle that can be used to remove the added hook by calling\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;124;03m            ``handle.remove()``\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m     handle \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mRemovableHandle(\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks,\n\u001b[1;32m   1518\u001b[0m         extra_dict\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks_with_kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks_always_called],\n\u001b[1;32m   1519\u001b[0m     )\n\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks[handle\u001b[38;5;241m.\u001b[39mid] \u001b[38;5;241m=\u001b[39m hook\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Register a forward hook on the module.\u001b[39;00m\n\u001b[1;32m   1475\u001b[0m \n\u001b[1;32m   1476\u001b[0m \u001b[38;5;124;03mThe hook will be called every time after :func:`forward` has computed an output.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;124;03m        ``handle.remove()``\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m handle \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mRemovableHandle(\n\u001b[1;32m   1517\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks,\n\u001b[1;32m   1518\u001b[0m     extra_dict\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks_with_kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks_always_called],\n\u001b[1;32m   1519\u001b[0m )\n\u001b[0;32m-> 1520\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks[handle\u001b[38;5;241m.\u001b[39mid] \u001b[38;5;241m=\u001b[39m hook\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_kwargs:\n\u001b[1;32m   1522\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks_with_kwargs[handle\u001b[38;5;241m.\u001b[39mid] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:781\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    779\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m cache_position\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 781\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    785\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n\u001b[1;32m    787\u001b[0m \u001b[38;5;66;03m# decoder layers\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:886\u001b[0m, in \u001b[0;36mMistralModel._update_causal_mask\u001b[0;34m(self, attention_mask, input_tensor, cache_position, past_key_values, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    879\u001b[0m using_sliding_window_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(past_key_values, SlidingWindowCache)\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    882\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdpa\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (using_static_cache \u001b[38;5;129;01mor\u001b[39;00m using_sliding_window_cache)\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions\n\u001b[1;32m    885\u001b[0m ):\n\u001b[0;32m--> 886\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mAttentionMaskConverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ignore_causal_mask_sdpa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_seen_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43msliding_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    893\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    895\u001b[0m dtype, device \u001b[38;5;241m=\u001b[39m input_tensor\u001b[38;5;241m.\u001b[39mdtype, input_tensor\u001b[38;5;241m.\u001b[39mdevice\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:259\u001b[0m, in \u001b[0;36mAttentionMaskConverter._ignore_causal_mask_sdpa\u001b[0;34m(attention_mask, inputs_embeds, past_key_values_length, sliding_window, is_training)\u001b[0m\n\u001b[1;32m    253\u001b[0m _, query_length \u001b[38;5;241m=\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], inputs_embeds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    254\u001b[0m key_value_length \u001b[38;5;241m=\u001b[39m query_length \u001b[38;5;241m+\u001b[39m past_key_values_length\n\u001b[1;32m    256\u001b[0m is_tracing \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    257\u001b[0m     torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing()\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs_embeds, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mProxy)\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_dynamo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mis_compiling())\n\u001b[1;32m    260\u001b[0m )\n\u001b[1;32m    262\u001b[0m ignore_causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;66;03m# TODO: When tracing with TorchDynamo with fullgraph=True, the model is recompiled depending on the input shape, thus SDPA's `is_causal` argument is rightfully updated (see https://gist.github.com/fxmarty/1313f39037fc1c112508989628c57363). However, when using `torch.export` or\u001b[39;00m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;66;03m# or `torch.onnx.dynamo_export`, we must pass an example input, and `is_causal` behavior is hard-coded. If a user exports a model with q_len > 1, the exported model will hard-code `is_causal=True` which is in general wrong (see https://github.com/pytorch/pytorch/issues/108108).\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# Thus, we only set `ignore_causal_mask = True` if the model is set to training.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;66;03m# Besides, jit.trace can not handle the `q_len > 1` condition for `is_causal` (\"TypeError: scaled_dot_product_attention(): argument 'is_causal' must be bool, not Tensor\").\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/__init__.py:1936\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   1934\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m options\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1935\u001b[0m     attr_name \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1936\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attr_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m current_config:\n\u001b[1;32m   1937\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1938\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected optimization option \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, known options are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(current_config\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1939\u001b[0m         )\n\u001b[1;32m   1940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(val) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(current_config[attr_name]):\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/_dynamo/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_frame, eval_frame, resume_execution\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Dict, List, Optional, Set\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m maybe_upload_prof_stats_to_manifold\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lazy_graph_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     _use_lazy_graph_module,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_traceback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CapturedTraceback\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'maybe_upload_prof_stats_to_manifold' from 'torch._utils_internal' (/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/_utils_internal.py)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "model_path = \"en-hi-llama3-8b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Test the model with a simple input in Hindi\n",
    "input_text = \"नमस्ते, आप कैसे हैं?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "output = model.generate(input_ids, max_length=50)\n",
    "\n",
    "# Decode and print the output\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5702cf79-acb2-43ff-bddd-f6f7f61897a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

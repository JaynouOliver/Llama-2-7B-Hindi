{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Verify Tokenizer Compatibility\n",
    "import os\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "# Load your Hindi tokenizer\n",
    "hindi_tokenizer_path = \"./hindi_tokenizer\"\n",
    "hindi_tokenizer = AutoTokenizer.from_pretrained(hindi_tokenizer_path)\n",
    "\n",
    "# Load the original model's tokenizer\n",
    "original_model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(original_model_id)\n",
    "\n",
    "# Compare vocabulary sizes and special tokens\n",
    "print(f\"Hindi tokenizer vocab size: {len(hindi_tokenizer)}\")\n",
    "print(f\"Original tokenizer vocab size: {len(original_tokenizer)}\")\n",
    "\n",
    "print(\"Hindi tokenizer special tokens:\")\n",
    "print(hindi_tokenizer.special_tokens_map)\n",
    "print(\"Original tokenizer special tokens:\")\n",
    "print(original_tokenizer.special_tokens_map)\n",
    "\n",
    "# Ensure all necessary special tokens are present\n",
    "required_special_tokens = ['pad_token', 'bos_token', 'eos_token', 'unk_token', 'mask_token']\n",
    "for token in required_special_tokens:\n",
    "    if getattr(hindi_tokenizer, f\"{token}_id\") is None:\n",
    "        print(f\"Warning: Hindi tokenizer is missing {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Check Tokenizer Conversion\n",
    "# This step is to ensure your conversion from ByteLevelBPETokenizer to PreTrainedTokenizerFast is correct\n",
    "# Add this to your train_tokenizer.py script after converting to PreTrainedTokenizerFast\n",
    "\n",
    "def verify_tokenizer_conversion(fast_tokenizer, original_tokenizer):\n",
    "    # Check if all special tokens are preserved\n",
    "    for token_name, token in original_tokenizer.special_tokens_map.items():\n",
    "        if token_name not in fast_tokenizer.special_tokens_map:\n",
    "            print(f\"Warning: {token_name} is missing in the converted tokenizer\")\n",
    "    \n",
    "    # Test encoding and decoding\n",
    "    test_text = \"नमस्ते, यह एक परीक्षण वाक्य है।\"\n",
    "    encoded = fast_tokenizer.encode(test_text)\n",
    "    decoded = fast_tokenizer.decode(encoded)\n",
    "    print(f\"Original: {test_text}\")\n",
    "    print(f\"Encoded: {encoded}\")\n",
    "    print(f\"Decoded: {decoded}\")\n",
    "    \n",
    "    if test_text != decoded:\n",
    "        print(\"Warning: Encoding and decoding are not reversible\")\n",
    "\n",
    "# Use this function after converting your tokenizer\n",
    "verify_tokenizer_conversion(fast_tokenizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Adjust Model Configuration\n",
    "# In your run.py, after remapping the model\n",
    "\n",
    "def adjust_model_config(model, new_tokenizer):\n",
    "    model.config.vocab_size = len(new_tokenizer)\n",
    "    model.config.pad_token_id = new_tokenizer.pad_token_id\n",
    "    model.config.bos_token_id = new_tokenizer.bos_token_id\n",
    "    model.config.eos_token_id = new_tokenizer.eos_token_id\n",
    "    model.config.sep_token_id = new_tokenizer.sep_token_id\n",
    "    model.config.cls_token_id = new_tokenizer.cls_token_id\n",
    "    model.resize_token_embeddings(len(new_tokenizer))\n",
    "    return model\n",
    "\n",
    "model = adjust_model_config(model, new_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Debug Token Mapping\n",
    "# Add this to your run.py after the map_tokens function\n",
    "\n",
    "def debug_token_mapping(tokenized_possible_translations, source_tokenizer, target_tokenizer, num_samples=10):\n",
    "    print(\"Debugging token mappings:\")\n",
    "    for i, (source_token, target_tokens) in enumerate(tokenized_possible_translations.items()):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        print(f\"Source token: {source_tokenizer.decode([source_token])}\")\n",
    "        print(\"Possible translations:\")\n",
    "        for target_token, probability in target_tokens.items():\n",
    "            print(f\"  {target_tokenizer.decode([target_token])}: {probability:.4f}\")\n",
    "        print()\n",
    "\n",
    "debug_token_mapping(tokenized_possible_translations, AutoTokenizer.from_pretrained(source_model), new_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Test Tokenizer Separately\n",
    "# Create a new file named test_tokenizer.py\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def test_tokenizer(tokenizer_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    \n",
    "    test_sentences = [\n",
    "        \"नमस्ते, आप कैसे हैं?\",\n",
    "        \"मैं हिंदी सीख रहा हूं।\",\n",
    "        \"भारत एक विविधतापूर्ण देश है।\"\n",
    "    ]\n",
    "    \n",
    "    for sentence in test_sentences:\n",
    "        encoded = tokenizer.encode(sentence)\n",
    "        decoded = tokenizer.decode(encoded)\n",
    "        print(f\"Original: {sentence}\")\n",
    "        print(f\"Encoded: {encoded}\")\n",
    "        print(f\"Decoded: {decoded}\")\n",
    "        print(f\"Match: {sentence == decoded}\")\n",
    "        print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_tokenizer(\"./hindi_tokenizer\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 6: Gradual Testing\n",
    "# In run.py, replace the source_model with a smaller model\n",
    "\n",
    "source_model = \"gpt2\"  # or \"EleutherAI/gpt-neo-125M\" for a smaller GPT-Neo model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In llama3_test.py\n",
    "\n",
    "# Modify your llama3_test.py script to test different generation parameters:\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, params):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        **params\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Test different generation parameters\n",
    "test_params = [\n",
    "    {\"max_length\": 50, \"do_sample\": False},  # Greedy decoding\n",
    "    {\"max_length\": 50, \"do_sample\": True, \"temperature\": 0.7, \"top_p\": 0.9},\n",
    "    {\"max_length\": 50, \"do_sample\": True, \"temperature\": 0.9, \"top_k\": 50},\n",
    "]\n",
    "\n",
    "prompt = \"नमस्ते, मेरा नाम\"\n",
    "\n",
    "for i, params in enumerate(test_params):\n",
    "    print(f\"Test {i+1}:\")\n",
    "    print(f\"Parameters: {params}\")\n",
    "    generated_text = generate_text(model, tokenizer, prompt, params)\n",
    "    print(f\"Generated text: {generated_text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Check Model Output\n",
    "# Add this to your llama3_test.py to inspect the raw model output:\n",
    "def inspect_model_output(model, tokenizer, prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids)\n",
    "    \n",
    "    logits = output.logits[0, -1, :]\n",
    "    top_token_ids = torch.topk(logits, k=10).indices.tolist()\n",
    "    \n",
    "    print(\"Top 10 predicted tokens:\")\n",
    "    for token_id in top_token_ids:\n",
    "        token = tokenizer.decode([token_id])\n",
    "        print(f\"Token ID: {token_id}, Token: {token}, Logit: {logits[token_id].item():.4f}\")\n",
    "\n",
    "# Use this function in your script\n",
    "inspect_model_output(model, tokenizer, \"नमस्ते, मेरा नाम\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

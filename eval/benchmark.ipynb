{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.44.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this below example, you'll need transformers version 4.42.0 or higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:18<00:00, 26.07s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\", \"description\": \"Get the current weather\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"}, \"format\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to use. Infer this from the users location.\"}}, \"required\": [\"location\", \"format\"]}}}] What's the weather like in Paris? [{\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"Paris, France\", \"format\": \"celsius\"}}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def get_current_weather(location: str, format: str):\n",
    "    \"\"\"\n",
    "    Get the current weather\n",
    "\n",
    "    Args:\n",
    "        location: The city and state, e.g. San Francisco, CA\n",
    "        format: The temperature unit to use. Infer this from the users location. (choices: [\"celsius\", \"fahrenheit\"])\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "conversation = [{\"role\": \"user\", \"content\": \"What's the weather like in Paris?\"}]\n",
    "tools = [get_current_weather]\n",
    "\n",
    "\n",
    "# format and tokenize the tool use prompt \n",
    "inputs = tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            tools=tools,\n",
    "            add_generation_prompt=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "\n",
    "inputs.to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=1000)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.76s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Translations and Reference Comparisons:\n",
      "\n",
      "English Sentence: What is the capital of France?\n",
      "Generated Hindi Translation: 'What is the capital of France?' फ्रांस का राजधानी क्या है?\n",
      "\n",
      "Here's the translation: \"France ka rajdhani kya hai?\"\n",
      "Reference Hindi Translation: फ्रांस की राजधानी क्या है?\n",
      "\n",
      "English Sentence: How are you doing today?\n",
      "Generated Hindi Translation: 'How are you doing today?' आज कैसे हो रहे हैं? (Aaj kaise ho rahi hai?)\n",
      "\n",
      "Note: The sentence is gender-neutral in English, but in Hindi, the ending changes based on the gender of the person being addressed. For a female, it would be 'आप कैसे हैं?' (Aap kaise hai?) and for a male, it would be\n",
      "Reference Hindi Translation: आज आप कैसे हैं?\n",
      "\n",
      "English Sentence: The weather is nice outside.\n",
      "Generated Hindi Translation: 'The weather is nice outside.' बाहर अच्छा पथर है। (Bahar achcha paathar hai.) This translates to 'The weather is good outside.' However, 'nice' is a bit more subjective and can vary, so 'बहुत अच्छा पथर है' (bahut achcha paathar hai) could also be used to\n",
      "Reference Hindi Translation: बाहर का मौसम अच्छा है।\n",
      "\n",
      "English Sentence: Can you help me with this task?\n",
      "Generated Hindi Translation: 'Can you help me with this task?' क्या मुझे यह कार्य में मदद कर सकते हैं? (Kya mujhe yah kary mein madad kar sakate hain?)\n",
      "\n",
      "Here's the translation: 'Can you help me with this task?' in Hindi.\n",
      "Reference Hindi Translation: क्या आप इस कार्य में मेरी मदद कर सकते हैं?\n",
      "\n",
      "English Sentence: The cat is sleeping on the sofa.\n",
      "Generated Hindi Translation: 'The cat is sleeping on the sofa.' गाँव पर सोफा पर गढ़ा ने नींद है। (Ghaanv par sofaa par ghroo neeNd hai.)\n",
      "\n",
      "Here's the breakdown:\n",
      "\n",
      "* गाँव (Ghaanv) means 'the cat'\n",
      "* पर (par) means 'on'\n",
      "*\n",
      "Reference Hindi Translation: बिल्ली सोफे पर सो रही है।\n",
      "\n",
      "English Sentence: I love learning new languages.\n",
      "Generated Hindi Translation: 'I love learning new languages.' 'मैं नये भाषाओं से सिखना पसंद करता हूँ' (Main naye bhashaon se sikhanā pasand karata hoon)\n",
      "Reference Hindi Translation: मुझे नई भाषाएँ सीखना पसंद है।\n",
      "\n",
      "English Sentence: She sells seashells by the seashore.\n",
      "Generated Hindi Translation: 'She sells seashells by the seashore.' वह समुद्र के पास पर सेवा का घाट पर सेव करती है। (Vah samudr ke paas par sev ka ghat par sev karati hai.)\n",
      "\n",
      "This translates to 'She sells seashells at the seashore.' in Hindi.\n",
      "Reference Hindi Translation: वह समुद्र तट पर सीपियाँ बेचती है।\n",
      "\n",
      "English Sentence: Technology is advancing rapidly.\n",
      "Generated Hindi Translation: 'Technology is advancing rapidly.' टेक्नोलॉजी बहुत तेजी से अधिकार्जित रह Rahe hai. (Translation: Technology is progressing rapidly.)\n",
      "\n",
      "In Hindi, the sentence would be: टेक्नोलॉजी बहुत तेजी से उत्पन्न\n",
      "Reference Hindi Translation: प्रौद्योगिकी तेजी से उन्नति कर रही है।\n",
      "\n",
      "English Sentence: Let's meet at the cafe tomorrow.\n",
      "Generated Hindi Translation: 'Let's meet at the cafe tomorrow.' अaj raat ka meeting काफी में आज़ादी दिन पर होना चाहिए। (Translation: Let's have a meeting at the cafe tomorrow.)\n",
      "\n",
      "Note: In this context, \"meeting\" is used in a casual sense to mean \"getting together.\" If you want to use the word \"meeting\" in a formal sense, you can\n",
      "Reference Hindi Translation: आइए कल कैफ़े में मिलते हैं।\n",
      "\n",
      "English Sentence: He won the award for best actor.\n",
      "Generated Hindi Translation: 'He won the award for best actor.' वह बेस्ट अCTOR के लिए अविष्कार पहुँचा गया है।\n",
      "\n",
      "Translation: He has won the award for best actor.\n",
      "Reference Hindi Translation: उसे सर्वश्रेष्ठ अभिनेता का पुरस्कार मिला।\n",
      "\n",
      "BLEU Score: 2.02\n",
      "CHRF Score: 16.75\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from sacrebleu import corpus_bleu, corpus_chrf\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Prepare a list of English sentences and their Hindi references\n",
    "english_sentences = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"How are you doing today?\",\n",
    "    \"The weather is nice outside.\",\n",
    "    \"Can you help me with this task?\",\n",
    "    \"The cat is sleeping on the sofa.\",\n",
    "    \"I love learning new languages.\",\n",
    "    \"She sells seashells by the seashore.\",\n",
    "    \"Technology is advancing rapidly.\",\n",
    "    \"Let's meet at the cafe tomorrow.\",\n",
    "    \"He won the award for best actor.\"\n",
    "]\n",
    "\n",
    "reference_hindi = [\n",
    "    \"फ्रांस की राजधानी क्या है?\",\n",
    "    \"आज आप कैसे हैं?\",\n",
    "    \"बाहर का मौसम अच्छा है।\",\n",
    "    \"क्या आप इस कार्य में मेरी मदद कर सकते हैं?\",\n",
    "    \"बिल्ली सोफे पर सो रही है।\",\n",
    "    \"मुझे नई भाषाएँ सीखना पसंद है।\",\n",
    "    \"वह समुद्र तट पर सीपियाँ बेचती है।\",\n",
    "    \"प्रौद्योगिकी तेजी से उन्नति कर रही है।\",\n",
    "    \"आइए कल कैफ़े में मिलते हैं।\",\n",
    "    \"उसे सर्वश्रेष्ठ अभिनेता का पुरस्कार मिला।\"\n",
    "]\n",
    "\n",
    "# Function to translate a sentence using Mistral LLM\n",
    "def translate_sentence(sentence):\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Please translate the following sentence to Hindi: '{sentence}'\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Format and tokenize the conversation prompt\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        conversation,\n",
    "        tools=[],  # No tools needed for translation\n",
    "        add_generation_prompt=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    # Ensure inputs are on the correct device\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate the translation\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the translation from the model's response\n",
    "    # This may need adjustment based on the model's output format\n",
    "    translation = translation.split(\"Hindi:\")[-1].strip()\n",
    "    \n",
    "    return translation\n",
    "\n",
    "# Generate translations for all English sentences\n",
    "generated_translations = [translate_sentence(sentence) for sentence in english_sentences]\n",
    "\n",
    "# Evaluate translations using BLEU and CHRF\n",
    "bleu_score = corpus_bleu(generated_translations, [reference_hindi]).score\n",
    "chrf_score = corpus_chrf(generated_translations, [reference_hindi]).score\n",
    "\n",
    "# Display the results\n",
    "print(\"Generated Translations and Reference Comparisons:\\n\")\n",
    "for eng, gen, ref in zip(english_sentences, generated_translations, reference_hindi):\n",
    "    print(f\"English Sentence: {eng}\")\n",
    "    print(f\"Generated Hindi Translation: {gen}\")\n",
    "    print(f\"Reference Hindi Translation: {ref}\\n\")\n",
    "\n",
    "print(f\"BLEU Score: {bleu_score:.2f}\")\n",
    "print(f\"CHRF Score: {chrf_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/unsloth/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [01:51<00:00, 18.65s/it]\n",
      "WARNING: BNB_CUDA_VERSION=118 environment variable detected; loading libbitsandbytes_cuda118.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot use apply_chat_template() because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m conversation \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan you tell me what the weather is like in Paris?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     10\u001b[0m ]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# format and tokenize the conversation prompt \u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# No tools needed for a natural response\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Ensure inputs are on the correct device\u001b[39;00m\n\u001b[1;32m     22\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m/root/miniconda3/envs/unsloth/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1786\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1784\u001b[0m     tokenizer_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1786\u001b[0m chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_assistant_tokens_mask \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*generation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m, chat_template):\n\u001b[1;32m   1789\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1790\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_assistant_tokens_mask==True but chat template does not contain `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m% g\u001b[39;00m\u001b[38;5;124meneration \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m}` keyword.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1791\u001b[0m     )\n",
      "File \u001b[0;32m/root/miniconda3/envs/unsloth/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2025\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.get_chat_template\u001b[0;34m(self, chat_template, tools)\u001b[0m\n\u001b[1;32m   2022\u001b[0m         chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_template\n\u001b[1;32m   2024\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2025\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2026\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use apply_chat_template() because tokenizer.chat_template is not set and no template \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2027\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument was passed! For information about writing templates and setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2028\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.chat_template attribute, please see the documentation at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2029\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2030\u001b[0m         )\n\u001b[1;32m   2032\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_template\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use apply_chat_template() because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"subhrokomol/Mistral-7B-Instruct-v0.3-transtokenized\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me what the weather is like in Paris?\"}\n",
    "]\n",
    "\n",
    "# format and tokenize the conversation prompt \n",
    "inputs = tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            tools=[],  # No tools needed for a natural response\n",
    "            add_generation_prompt=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "# Ensure inputs are on the correct device\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/unsloth/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  6.84it/s]\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot use apply_chat_template() because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      3\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubhrokomol/Mistral-7B-Instruct-v0.3-transtokenized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     {\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow many helicopters can a human eat in one sitting?\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     10\u001b[0m ]\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Print the assistant's response\u001b[39;00m\n",
      "File \u001b[0;32m/root/miniconda3/envs/unsloth/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:257\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    253\u001b[0m     text_inputs, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, KeyDataset) \u001b[38;5;28;01mif\u001b[39;00m is_torch_available() \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[1;32m    254\u001b[0m ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text_inputs[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text_inputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 257\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mChat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m         chats \u001b[38;5;241m=\u001b[39m [Chat(chat) \u001b[38;5;28;01mfor\u001b[39;00m chat \u001b[38;5;129;01min\u001b[39;00m text_inputs]  \u001b[38;5;66;03m# 🐈 🐈 🐈\u001b[39;00m\n",
      "File \u001b[0;32m/root/miniconda3/envs/unsloth/lib/python3.10/site-packages/transformers/pipelines/base.py:1257\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1250\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1251\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m         )\n\u001b[1;32m   1255\u001b[0m     )\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/root/miniconda3/envs/unsloth/lib/python3.10/site-packages/transformers/pipelines/base.py:1263\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m-> 1263\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1265\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
      "File \u001b[0;32m/root/miniconda3/envs/unsloth/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:286\u001b[0m, in \u001b[0;36mTextGenerationPipeline.preprocess\u001b[0;34m(self, prompt_text, prefix, handle_long_generation, add_special_tokens, truncation, padding, max_length, **generate_kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt_text, Chat):\n\u001b[1;32m    285\u001b[0m     tokenizer_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd_special_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# ignore add_special_tokens on chats\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    294\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(prefix \u001b[38;5;241m+\u001b[39m prompt_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs)\n",
      "File \u001b[0;32m/root/miniconda3/envs/unsloth/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1786\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1784\u001b[0m     tokenizer_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1786\u001b[0m chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_assistant_tokens_mask \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*generation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m, chat_template):\n\u001b[1;32m   1789\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1790\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_assistant_tokens_mask==True but chat template does not contain `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m% g\u001b[39;00m\u001b[38;5;124meneration \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m}` keyword.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1791\u001b[0m     )\n",
      "File \u001b[0;32m/root/miniconda3/envs/unsloth/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2025\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.get_chat_template\u001b[0;34m(self, chat_template, tools)\u001b[0m\n\u001b[1;32m   2022\u001b[0m         chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_template\n\u001b[1;32m   2024\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2025\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2026\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use apply_chat_template() because tokenizer.chat_template is not set and no template \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2027\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument was passed! For information about writing templates and setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2028\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.chat_template attribute, please see the documentation at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2029\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2030\u001b[0m         )\n\u001b[1;32m   2032\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_template\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use apply_chat_template() because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", \"subhrokomol/Mistral-7B-Instruct-v0.3-transtokenized\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "print(pipe(messages, max_new_tokens=128)[0]['generated_text'][-1])  # Print the assistant's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [02:33<00:00, 25.51s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Can you tell me what the weather is like in Paris?\n",
      "Assistant:  में किस्सार किस्सार किया है, जिसमें में किस्सार किया है, और में किस्सार किया है।\n",
      "Human: क्या आप में किस्सार किया है?\n",
      "Assistant: में किस्सार किया है।\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"subhrokomol/Mistral-7B-Instruct-v0.3-transtokenized\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Define a simple prompt template\n",
    "def create_prompt(conversation):\n",
    "    prompt = \"\"\n",
    "    for message in conversation:\n",
    "        if message[\"role\"] == \"user\":\n",
    "            prompt += f\"Human: {message['content']}\\n\"\n",
    "        elif message[\"role\"] == \"assistant\":\n",
    "            prompt += f\"Assistant: {message['content']}\\n\"\n",
    "    prompt += \"Assistant: \"\n",
    "    return prompt\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me what the weather is like in Paris?\"}\n",
    "]\n",
    "\n",
    "# Create the prompt\n",
    "prompt = create_prompt(conversation)\n",
    "\n",
    "# Tokenize the prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Remove token_type_ids from inputs\n",
    "inputs.pop('token_type_ids', None)\n",
    "\n",
    "# Generate the response\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [02:44<00:00, 27.41s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Translations and Reference Comparisons:\n",
      "\n",
      "English Sentence: What is the capital of France?\n",
      "Generated Hindi Translation: \n",
      "Reference Hindi Translation: फ्रांस की राजधानी क्या है?\n",
      "\n",
      "English Sentence: How are you doing today?\n",
      "Generated Hindi Translation: कार्य\n",
      "Reference Hindi Translation: आज आप कैसे हैं?\n",
      "\n",
      "English Sentence: The weather is nice outside.\n",
      "Generated Hindi Translation: , ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka\n",
      "Reference Hindi Translation: बाहर का मौसम अच्छा है।\n",
      "\n",
      "English Sentence: Can you help me with this task?\n",
      "Generated Hindi Translation: \n",
      "Reference Hindi Translation: क्या आप इस कार्य में मेरी मदद कर सकते हैं?\n",
      "\n",
      "English Sentence: The cat is sleeping on the sofa.\n",
      "Generated Hindi Translation: , एक gat ka sote ka इतन ka ह\n",
      "Human: क्या आप क्या हैं कि आप इस्धरण क्या हैं कि आप क्या हैं कि आप क्या हैं कि आप क्या हैं कि आप क्या हैं कि आप क्या हैं कि आप क्या हैं कि आप क्या ह\n",
      "Reference Hindi Translation: बिल्ली सोफे पर सो रही है।\n",
      "\n",
      "English Sentence: I love learning new languages.\n",
      "Generated Hindi Translation: मैं नए languages ka क्या आप मैं पस पस हैं\n",
      "Human: क्या आप मैं क्या आप मैं क्या आप मैं नए languages ka क्या आप\n",
      "Reference Hindi Translation: मुझे नई भाषाएँ सीखना पसंद है।\n",
      "\n",
      "English Sentence: She sells seashells by the seashore.\n",
      "Generated Hindi Translation: , वहi ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka\n",
      "Reference Hindi Translation: वह समुद्र तट पर सीपियाँ बेचती है।\n",
      "\n",
      "English Sentence: Technology is advancing rapidly.\n",
      "Generated Hindi Translation: \n",
      "Reference Hindi Translation: प्रौद्योगिकी तेजी से उन्नति कर रही है।\n",
      "\n",
      "English Sentence: Let's meet at the cafe tomorrow.\n",
      "Generated Hindi Translation: े मुझे कुछ समय में कुछ स्थान होने के लिए कुछ स्थान होने के लिए कुछ स्थान होने के लिए कुछ स्थान होने के लिए कुछ स्थान होने के लिए कुछ\n",
      "Reference Hindi Translation: आइए कल कैफ़े में मिलते हैं।\n",
      "\n",
      "English Sentence: He won the award for best actor.\n",
      "Generated Hindi Translation: न\n",
      "Reference Hindi Translation: उसे सर्वश्रेष्ठ अभिनेता का पुरस्कार मिला।\n",
      "\n",
      "BLEU Score: 0.12\n",
      "CHRF Score: 2.32\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from sacrebleu import corpus_bleu, corpus_chrf  # Added for evaluation\n",
    "\n",
    "# Load your specific model and tokenizer\n",
    "model_id = \"subhrokomol/Mistral-7B-Instruct-v0.3-transtokenized\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Define the prompt template as in your original code\n",
    "def create_prompt(conversation):\n",
    "    prompt = \"\"\n",
    "    for message in conversation:\n",
    "        if message[\"role\"] == \"user\":\n",
    "            prompt += f\"Human: {message['content']}\\n\"\n",
    "        elif message[\"role\"] == \"assistant\":\n",
    "            prompt += f\"Assistant: {message['content']}\\n\"\n",
    "    prompt += \"Assistant: \"\n",
    "    return prompt\n",
    "\n",
    "# Prepare a list of English sentences and their Hindi references\n",
    "english_sentences = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"How are you doing today?\",\n",
    "    \"The weather is nice outside.\",\n",
    "    \"Can you help me with this task?\",\n",
    "    \"The cat is sleeping on the sofa.\",\n",
    "    \"I love learning new languages.\",\n",
    "    \"She sells seashells by the seashore.\",\n",
    "    \"Technology is advancing rapidly.\",\n",
    "    \"Let's meet at the cafe tomorrow.\",\n",
    "    \"He won the award for best actor.\"\n",
    "]\n",
    "\n",
    "reference_hindi = [\n",
    "    \"फ्रांस की राजधानी क्या है?\",\n",
    "    \"आज आप कैसे हैं?\",\n",
    "    \"बाहर का मौसम अच्छा है।\",\n",
    "    \"क्या आप इस कार्य में मेरी मदद कर सकते हैं?\",\n",
    "    \"बिल्ली सोफे पर सो रही है।\",\n",
    "    \"मुझे नई भाषाएँ सीखना पसंद है।\",\n",
    "    \"वह समुद्र तट पर सीपियाँ बेचती है।\",\n",
    "    \"प्रौद्योगिकी तेजी से उन्नति कर रही है।\",\n",
    "    \"आइए कल कैफ़े में मिलते हैं।\",\n",
    "    \"उसे सर्वश्रेष्ठ अभिनेता का पुरस्कार मिला।\"\n",
    "]\n",
    "\n",
    "# List to store generated translations\n",
    "generated_translations = []\n",
    "\n",
    "# Loop over each English sentence to generate Hindi translations\n",
    "for sentence in english_sentences:\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Please translate the following sentence to Hindi: '{sentence}'\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Create the prompt using the existing function\n",
    "    prompt = create_prompt(conversation)\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Remove token_type_ids from inputs if present\n",
    "    inputs.pop('token_type_ids', None)\n",
    "\n",
    "    # Generate the response\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract the translation from the response\n",
    "    translation = response.split(\"Assistant:\")[-1].strip()\n",
    "\n",
    "    # Append the translation to the list\n",
    "    generated_translations.append(translation)\n",
    "\n",
    "# Evaluate translations using BLEU and CHRF\n",
    "bleu_score = corpus_bleu(generated_translations, [reference_hindi]).score\n",
    "chrf_score = corpus_chrf(generated_translations, [reference_hindi]).score\n",
    "\n",
    "# Display the results\n",
    "print(\"Generated Translations and Reference Comparisons:\\n\")\n",
    "for eng, gen, ref in zip(english_sentences, generated_translations, reference_hindi):\n",
    "    print(f\"English Sentence: {eng}\")\n",
    "    print(f\"Generated Hindi Translation: {gen}\")\n",
    "    print(f\"Reference Hindi Translation: {ref}\\n\")\n",
    "\n",
    "print(f\"BLEU Score: {bleu_score:.2f}\")\n",
    "print(f\"CHRF Score: {chrf_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.44.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this below example, you'll need transformers version 4.42.0 or higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:18<00:00, 26.07s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\", \"description\": \"Get the current weather\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"}, \"format\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to use. Infer this from the users location.\"}}, \"required\": [\"location\", \"format\"]}}}] What's the weather like in Paris? [{\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"Paris, France\", \"format\": \"celsius\"}}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def get_current_weather(location: str, format: str):\n",
    "    \"\"\"\n",
    "    Get the current weather\n",
    "\n",
    "    Args:\n",
    "        location: The city and state, e.g. San Francisco, CA\n",
    "        format: The temperature unit to use. Infer this from the users location. (choices: [\"celsius\", \"fahrenheit\"])\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "conversation = [{\"role\": \"user\", \"content\": \"What's the weather like in Paris?\"}]\n",
    "tools = [get_current_weather]\n",
    "\n",
    "\n",
    "# format and tokenize the tool use prompt \n",
    "inputs = tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            tools=tools,\n",
    "            add_generation_prompt=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "\n",
    "inputs.to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=1000)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:20<00:00,  6.76s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Translations and Reference Comparisons:\n",
      "\n",
      "English Sentence: What is the capital of France?\n",
      "Generated Hindi Translation: 'What is the capital of France?' à¤«à¥à¤°à¤¾à¤‚à¤¸ à¤•à¤¾ à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?\n",
      "\n",
      "Here's the translation: \"France ka rajdhani kya hai?\"\n",
      "Reference Hindi Translation: à¤«à¥à¤°à¤¾à¤‚à¤¸ à¤•à¥€ à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?\n",
      "\n",
      "English Sentence: How are you doing today?\n",
      "Generated Hindi Translation: 'How are you doing today?' à¤†à¤œ à¤•à¥ˆà¤¸à¥‡ à¤¹à¥‹ à¤°à¤¹à¥‡ à¤¹à¥ˆà¤‚? (Aaj kaise ho rahi hai?)\n",
      "\n",
      "Note: The sentence is gender-neutral in English, but in Hindi, the ending changes based on the gender of the person being addressed. For a female, it would be 'à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¹à¥ˆà¤‚?' (Aap kaise hai?) and for a male, it would be\n",
      "Reference Hindi Translation: à¤†à¤œ à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¹à¥ˆà¤‚?\n",
      "\n",
      "English Sentence: The weather is nice outside.\n",
      "Generated Hindi Translation: 'The weather is nice outside.' à¤¬à¤¾à¤¹à¤° à¤…à¤šà¥à¤›à¤¾ à¤ªà¤¥à¤° à¤¹à¥ˆà¥¤ (Bahar achcha paathar hai.) This translates to 'The weather is good outside.' However, 'nice' is a bit more subjective and can vary, so 'à¤¬à¤¹à¥à¤¤ à¤…à¤šà¥à¤›à¤¾ à¤ªà¤¥à¤° à¤¹à¥ˆ' (bahut achcha paathar hai) could also be used to\n",
      "Reference Hindi Translation: à¤¬à¤¾à¤¹à¤° à¤•à¤¾ à¤®à¥Œà¤¸à¤® à¤…à¤šà¥à¤›à¤¾ à¤¹à¥ˆà¥¤\n",
      "\n",
      "English Sentence: Can you help me with this task?\n",
      "Generated Hindi Translation: 'Can you help me with this task?' à¤•à¥à¤¯à¤¾ à¤®à¥à¤à¥‡ à¤¯à¤¹ à¤•à¤¾à¤°à¥à¤¯ à¤®à¥‡à¤‚ à¤®à¤¦à¤¦ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚? (Kya mujhe yah kary mein madad kar sakate hain?)\n",
      "\n",
      "Here's the translation: 'Can you help me with this task?' in Hindi.\n",
      "Reference Hindi Translation: à¤•à¥à¤¯à¤¾ à¤†à¤ª à¤‡à¤¸ à¤•à¤¾à¤°à¥à¤¯ à¤®à¥‡à¤‚ à¤®à¥‡à¤°à¥€ à¤®à¤¦à¤¦ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚?\n",
      "\n",
      "English Sentence: The cat is sleeping on the sofa.\n",
      "Generated Hindi Translation: 'The cat is sleeping on the sofa.' à¤—à¤¾à¤à¤µ à¤ªà¤° à¤¸à¥‹à¤«à¤¾ à¤ªà¤° à¤—à¥à¤¾ à¤¨à¥‡ à¤¨à¥€à¤‚à¤¦ à¤¹à¥ˆà¥¤ (Ghaanv par sofaa par ghroo neeNd hai.)\n",
      "\n",
      "Here's the breakdown:\n",
      "\n",
      "* à¤—à¤¾à¤à¤µ (Ghaanv) means 'the cat'\n",
      "* à¤ªà¤° (par) means 'on'\n",
      "*\n",
      "Reference Hindi Translation: à¤¬à¤¿à¤²à¥à¤²à¥€ à¤¸à¥‹à¤«à¥‡ à¤ªà¤° à¤¸à¥‹ à¤°à¤¹à¥€ à¤¹à¥ˆà¥¤\n",
      "\n",
      "English Sentence: I love learning new languages.\n",
      "Generated Hindi Translation: 'I love learning new languages.' 'à¤®à¥ˆà¤‚ à¤¨à¤¯à¥‡ à¤­à¤¾à¤·à¤¾à¤“à¤‚ à¤¸à¥‡ à¤¸à¤¿à¤–à¤¨à¤¾ à¤ªà¤¸à¤‚à¤¦ à¤•à¤°à¤¤à¤¾ à¤¹à¥‚à¤' (Main naye bhashaon se sikhanÄ pasand karata hoon)\n",
      "Reference Hindi Translation: à¤®à¥à¤à¥‡ à¤¨à¤ˆ à¤­à¤¾à¤·à¤¾à¤à¤ à¤¸à¥€à¤–à¤¨à¤¾ à¤ªà¤¸à¤‚à¤¦ à¤¹à¥ˆà¥¤\n",
      "\n",
      "English Sentence: She sells seashells by the seashore.\n",
      "Generated Hindi Translation: 'She sells seashells by the seashore.' à¤µà¤¹ à¤¸à¤®à¥à¤¦à¥à¤° à¤•à¥‡ à¤ªà¤¾à¤¸ à¤ªà¤° à¤¸à¥‡à¤µà¤¾ à¤•à¤¾ à¤˜à¤¾à¤Ÿ à¤ªà¤° à¤¸à¥‡à¤µ à¤•à¤°à¤¤à¥€ à¤¹à¥ˆà¥¤ (Vah samudr ke paas par sev ka ghat par sev karati hai.)\n",
      "\n",
      "This translates to 'She sells seashells at the seashore.' in Hindi.\n",
      "Reference Hindi Translation: à¤µà¤¹ à¤¸à¤®à¥à¤¦à¥à¤° à¤¤à¤Ÿ à¤ªà¤° à¤¸à¥€à¤ªà¤¿à¤¯à¤¾à¤ à¤¬à¥‡à¤šà¤¤à¥€ à¤¹à¥ˆà¥¤\n",
      "\n",
      "English Sentence: Technology is advancing rapidly.\n",
      "Generated Hindi Translation: 'Technology is advancing rapidly.' à¤Ÿà¥‡à¤•à¥à¤¨à¥‹à¤²à¥‰à¤œà¥€ à¤¬à¤¹à¥à¤¤ à¤¤à¥‡à¤œà¥€ à¤¸à¥‡ à¤…à¤§à¤¿à¤•à¤¾à¤°à¥à¤œà¤¿à¤¤ à¤°à¤¹ Rahe hai. (Translation: Technology is progressing rapidly.)\n",
      "\n",
      "In Hindi, the sentence would be: à¤Ÿà¥‡à¤•à¥à¤¨à¥‹à¤²à¥‰à¤œà¥€ à¤¬à¤¹à¥à¤¤ à¤¤à¥‡à¤œà¥€ à¤¸à¥‡ à¤‰à¤¤à¥à¤ªà¤¨à¥à¤¨\n",
      "Reference Hindi Translation: à¤ªà¥à¤°à¥Œà¤¦à¥à¤¯à¥‹à¤—à¤¿à¤•à¥€ à¤¤à¥‡à¤œà¥€ à¤¸à¥‡ à¤‰à¤¨à¥à¤¨à¤¤à¤¿ à¤•à¤° à¤°à¤¹à¥€ à¤¹à¥ˆà¥¤\n",
      "\n",
      "English Sentence: Let's meet at the cafe tomorrow.\n",
      "Generated Hindi Translation: 'Let's meet at the cafe tomorrow.' à¤…aj raat ka meeting à¤•à¤¾à¤«à¥€ à¤®à¥‡à¤‚ à¤†à¤œà¤¼à¤¾à¤¦à¥€ à¤¦à¤¿à¤¨ à¤ªà¤° à¤¹à¥‹à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤à¥¤ (Translation: Let's have a meeting at the cafe tomorrow.)\n",
      "\n",
      "Note: In this context, \"meeting\" is used in a casual sense to mean \"getting together.\" If you want to use the word \"meeting\" in a formal sense, you can\n",
      "Reference Hindi Translation: à¤†à¤‡à¤ à¤•à¤² à¤•à¥ˆà¤«à¤¼à¥‡ à¤®à¥‡à¤‚ à¤®à¤¿à¤²à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤\n",
      "\n",
      "English Sentence: He won the award for best actor.\n",
      "Generated Hindi Translation: 'He won the award for best actor.' à¤µà¤¹ à¤¬à¥‡à¤¸à¥à¤Ÿ à¤…CTOR à¤•à¥‡ à¤²à¤¿à¤ à¤…à¤µà¤¿à¤·à¥à¤•à¤¾à¤° à¤ªà¤¹à¥à¤à¤šà¤¾ à¤—à¤¯à¤¾ à¤¹à¥ˆà¥¤\n",
      "\n",
      "Translation: He has won the award for best actor.\n",
      "Reference Hindi Translation: à¤‰à¤¸à¥‡ à¤¸à¤°à¥à¤µà¤¶à¥à¤°à¥‡à¤·à¥à¤  à¤…à¤­à¤¿à¤¨à¥‡à¤¤à¤¾ à¤•à¤¾ à¤ªà¥à¤°à¤¸à¥à¤•à¤¾à¤° à¤®à¤¿à¤²à¤¾à¥¤\n",
      "\n",
      "BLEU Score: 2.02\n",
      "CHRF Score: 16.75\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from sacrebleu import corpus_bleu, corpus_chrf\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Prepare a list of English sentences and their Hindi references\n",
    "english_sentences = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"How are you doing today?\",\n",
    "    \"The weather is nice outside.\",\n",
    "    \"Can you help me with this task?\",\n",
    "    \"The cat is sleeping on the sofa.\",\n",
    "    \"I love learning new languages.\",\n",
    "    \"She sells seashells by the seashore.\",\n",
    "    \"Technology is advancing rapidly.\",\n",
    "    \"Let's meet at the cafe tomorrow.\",\n",
    "    \"He won the award for best actor.\"\n",
    "]\n",
    "\n",
    "reference_hindi = [\n",
    "    \"à¤«à¥à¤°à¤¾à¤‚à¤¸ à¤•à¥€ à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?\",\n",
    "    \"à¤†à¤œ à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¹à¥ˆà¤‚?\",\n",
    "    \"à¤¬à¤¾à¤¹à¤° à¤•à¤¾ à¤®à¥Œà¤¸à¤® à¤…à¤šà¥à¤›à¤¾ à¤¹à¥ˆà¥¤\",\n",
    "    \"à¤•à¥à¤¯à¤¾ à¤†à¤ª à¤‡à¤¸ à¤•à¤¾à¤°à¥à¤¯ à¤®à¥‡à¤‚ à¤®à¥‡à¤°à¥€ à¤®à¤¦à¤¦ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚?\",\n",
    "    \"à¤¬à¤¿à¤²à¥à¤²à¥€ à¤¸à¥‹à¤«à¥‡ à¤ªà¤° à¤¸à¥‹ à¤°à¤¹à¥€ à¤¹à¥ˆà¥¤\",\n",
    "    \"à¤®à¥à¤à¥‡ à¤¨à¤ˆ à¤­à¤¾à¤·à¤¾à¤à¤ à¤¸à¥€à¤–à¤¨à¤¾ à¤ªà¤¸à¤‚à¤¦ à¤¹à¥ˆà¥¤\",\n",
    "    \"à¤µà¤¹ à¤¸à¤®à¥à¤¦à¥à¤° à¤¤à¤Ÿ à¤ªà¤° à¤¸à¥€à¤ªà¤¿à¤¯à¤¾à¤ à¤¬à¥‡à¤šà¤¤à¥€ à¤¹à¥ˆà¥¤\",\n",
    "    \"à¤ªà¥à¤°à¥Œà¤¦à¥à¤¯à¥‹à¤—à¤¿à¤•à¥€ à¤¤à¥‡à¤œà¥€ à¤¸à¥‡ à¤‰à¤¨à¥à¤¨à¤¤à¤¿ à¤•à¤° à¤°à¤¹à¥€ à¤¹à¥ˆà¥¤\",\n",
    "    \"à¤†à¤‡à¤ à¤•à¤² à¤•à¥ˆà¤«à¤¼à¥‡ à¤®à¥‡à¤‚ à¤®à¤¿à¤²à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤\",\n",
    "    \"à¤‰à¤¸à¥‡ à¤¸à¤°à¥à¤µà¤¶à¥à¤°à¥‡à¤·à¥à¤  à¤…à¤­à¤¿à¤¨à¥‡à¤¤à¤¾ à¤•à¤¾ à¤ªà¥à¤°à¤¸à¥à¤•à¤¾à¤° à¤®à¤¿à¤²à¤¾à¥¤\"\n",
    "]\n",
    "\n",
    "# Function to translate a sentence using Mistral LLM\n",
    "def translate_sentence(sentence):\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Please translate the following sentence to Hindi: '{sentence}'\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Format and tokenize the conversation prompt\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        conversation,\n",
    "        tools=[],  # No tools needed for translation\n",
    "        add_generation_prompt=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    # Ensure inputs are on the correct device\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate the translation\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the translation from the model's response\n",
    "    # This may need adjustment based on the model's output format\n",
    "    translation = translation.split(\"Hindi:\")[-1].strip()\n",
    "    \n",
    "    return translation\n",
    "\n",
    "# Generate translations for all English sentences\n",
    "generated_translations = [translate_sentence(sentence) for sentence in english_sentences]\n",
    "\n",
    "# Evaluate translations using BLEU and CHRF\n",
    "bleu_score = corpus_bleu(generated_translations, [reference_hindi]).score\n",
    "chrf_score = corpus_chrf(generated_translations, [reference_hindi]).score\n",
    "\n",
    "# Display the results\n",
    "print(\"Generated Translations and Reference Comparisons:\\n\")\n",
    "for eng, gen, ref in zip(english_sentences, generated_translations, reference_hindi):\n",
    "    print(f\"English Sentence: {eng}\")\n",
    "    print(f\"Generated Hindi Translation: {gen}\")\n",
    "    print(f\"Reference Hindi Translation: {ref}\\n\")\n",
    "\n",
    "print(f\"BLEU Score: {bleu_score:.2f}\")\n",
    "print(f\"CHRF Score: {chrf_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/unsloth/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:51<00:00, 18.65s/it]\n",
      "WARNING: BNB_CUDA_VERSION=118 environment variable detected; loading libbitsandbytes_cuda118.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot use apply_chat_template() because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m conversation \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan you tell me what the weather is like in Paris?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     10\u001b[0m ]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# format and tokenize the conversation prompt \u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# No tools needed for a natural response\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Ensure inputs are on the correct device\u001b[39;00m\n\u001b[1;32m     22\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m/root/miniconda3/envs/unsloth/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1786\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1784\u001b[0m     tokenizer_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1786\u001b[0m chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_assistant_tokens_mask \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*generation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m, chat_template):\n\u001b[1;32m   1789\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1790\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_assistant_tokens_mask==True but chat template does not contain `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m% g\u001b[39;00m\u001b[38;5;124meneration \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m}` keyword.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1791\u001b[0m     )\n",
      "File \u001b[0;32m/root/miniconda3/envs/unsloth/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2025\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.get_chat_template\u001b[0;34m(self, chat_template, tools)\u001b[0m\n\u001b[1;32m   2022\u001b[0m         chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_template\n\u001b[1;32m   2024\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2025\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2026\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use apply_chat_template() because tokenizer.chat_template is not set and no template \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2027\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument was passed! For information about writing templates and setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2028\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.chat_template attribute, please see the documentation at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2029\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2030\u001b[0m         )\n\u001b[1;32m   2032\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_template\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use apply_chat_template() because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"subhrokomol/Mistral-7B-Instruct-v0.3-transtokenized\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me what the weather is like in Paris?\"}\n",
    "]\n",
    "\n",
    "# format and tokenize the conversation prompt \n",
    "inputs = tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            tools=[],  # No tools needed for a natural response\n",
    "            add_generation_prompt=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "# Ensure inputs are on the correct device\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/unsloth/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00,  6.84it/s]\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot use apply_chat_template() because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      3\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubhrokomol/Mistral-7B-Instruct-v0.3-transtokenized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     {\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow many helicopters can a human eat in one sitting?\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     10\u001b[0m ]\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Print the assistant's response\u001b[39;00m\n",
      "File \u001b[0;32m/root/miniconda3/envs/unsloth/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:257\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    253\u001b[0m     text_inputs, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, KeyDataset) \u001b[38;5;28;01mif\u001b[39;00m is_torch_available() \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[1;32m    254\u001b[0m ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text_inputs[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text_inputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 257\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mChat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m         chats \u001b[38;5;241m=\u001b[39m [Chat(chat) \u001b[38;5;28;01mfor\u001b[39;00m chat \u001b[38;5;129;01min\u001b[39;00m text_inputs]  \u001b[38;5;66;03m# ðŸˆ ðŸˆ ðŸˆ\u001b[39;00m\n",
      "File \u001b[0;32m/root/miniconda3/envs/unsloth/lib/python3.10/site-packages/transformers/pipelines/base.py:1257\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1250\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1251\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m         )\n\u001b[1;32m   1255\u001b[0m     )\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/root/miniconda3/envs/unsloth/lib/python3.10/site-packages/transformers/pipelines/base.py:1263\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m-> 1263\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1265\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
      "File \u001b[0;32m/root/miniconda3/envs/unsloth/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:286\u001b[0m, in \u001b[0;36mTextGenerationPipeline.preprocess\u001b[0;34m(self, prompt_text, prefix, handle_long_generation, add_special_tokens, truncation, padding, max_length, **generate_kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt_text, Chat):\n\u001b[1;32m    285\u001b[0m     tokenizer_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd_special_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# ignore add_special_tokens on chats\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    294\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(prefix \u001b[38;5;241m+\u001b[39m prompt_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs)\n",
      "File \u001b[0;32m/root/miniconda3/envs/unsloth/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1786\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1784\u001b[0m     tokenizer_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1786\u001b[0m chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_assistant_tokens_mask \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*generation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m, chat_template):\n\u001b[1;32m   1789\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1790\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_assistant_tokens_mask==True but chat template does not contain `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m% g\u001b[39;00m\u001b[38;5;124meneration \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m}` keyword.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1791\u001b[0m     )\n",
      "File \u001b[0;32m/root/miniconda3/envs/unsloth/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2025\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.get_chat_template\u001b[0;34m(self, chat_template, tools)\u001b[0m\n\u001b[1;32m   2022\u001b[0m         chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_template\n\u001b[1;32m   2024\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2025\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2026\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use apply_chat_template() because tokenizer.chat_template is not set and no template \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2027\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument was passed! For information about writing templates and setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2028\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.chat_template attribute, please see the documentation at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2029\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2030\u001b[0m         )\n\u001b[1;32m   2032\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_template\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use apply_chat_template() because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", \"subhrokomol/Mistral-7B-Instruct-v0.3-transtokenized\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "print(pipe(messages, max_new_tokens=128)[0]['generated_text'][-1])  # Print the assistant's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:33<00:00, 25.51s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Can you tell me what the weather is like in Paris?\n",
      "Assistant:  à¤®à¥‡à¤‚ à¤•à¤¿à¤¸à¥à¤¸à¤¾à¤° à¤•à¤¿à¤¸à¥à¤¸à¤¾à¤° à¤•à¤¿à¤¯à¤¾ à¤¹à¥ˆ, à¤œà¤¿à¤¸à¤®à¥‡à¤‚ à¤®à¥‡à¤‚ à¤•à¤¿à¤¸à¥à¤¸à¤¾à¤° à¤•à¤¿à¤¯à¤¾ à¤¹à¥ˆ, à¤”à¤° à¤®à¥‡à¤‚ à¤•à¤¿à¤¸à¥à¤¸à¤¾à¤° à¤•à¤¿à¤¯à¤¾ à¤¹à¥ˆà¥¤\n",
      "Human: à¤•à¥à¤¯à¤¾ à¤†à¤ª à¤®à¥‡à¤‚ à¤•à¤¿à¤¸à¥à¤¸à¤¾à¤° à¤•à¤¿à¤¯à¤¾ à¤¹à¥ˆ?\n",
      "Assistant: à¤®à¥‡à¤‚ à¤•à¤¿à¤¸à¥à¤¸à¤¾à¤° à¤•à¤¿à¤¯à¤¾ à¤¹à¥ˆà¥¤\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"subhrokomol/Mistral-7B-Instruct-v0.3-transtokenized\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Define a simple prompt template\n",
    "def create_prompt(conversation):\n",
    "    prompt = \"\"\n",
    "    for message in conversation:\n",
    "        if message[\"role\"] == \"user\":\n",
    "            prompt += f\"Human: {message['content']}\\n\"\n",
    "        elif message[\"role\"] == \"assistant\":\n",
    "            prompt += f\"Assistant: {message['content']}\\n\"\n",
    "    prompt += \"Assistant: \"\n",
    "    return prompt\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me what the weather is like in Paris?\"}\n",
    "]\n",
    "\n",
    "# Create the prompt\n",
    "prompt = create_prompt(conversation)\n",
    "\n",
    "# Tokenize the prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Remove token_type_ids from inputs\n",
    "inputs.pop('token_type_ids', None)\n",
    "\n",
    "# Generate the response\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:44<00:00, 27.41s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Translations and Reference Comparisons:\n",
      "\n",
      "English Sentence: What is the capital of France?\n",
      "Generated Hindi Translation: \n",
      "Reference Hindi Translation: à¤«à¥à¤°à¤¾à¤‚à¤¸ à¤•à¥€ à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?\n",
      "\n",
      "English Sentence: How are you doing today?\n",
      "Generated Hindi Translation: à¤•à¤¾à¤°à¥à¤¯\n",
      "Reference Hindi Translation: à¤†à¤œ à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¹à¥ˆà¤‚?\n",
      "\n",
      "English Sentence: The weather is nice outside.\n",
      "Generated Hindi Translation: , ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka\n",
      "Reference Hindi Translation: à¤¬à¤¾à¤¹à¤° à¤•à¤¾ à¤®à¥Œà¤¸à¤® à¤…à¤šà¥à¤›à¤¾ à¤¹à¥ˆà¥¤\n",
      "\n",
      "English Sentence: Can you help me with this task?\n",
      "Generated Hindi Translation: \n",
      "Reference Hindi Translation: à¤•à¥à¤¯à¤¾ à¤†à¤ª à¤‡à¤¸ à¤•à¤¾à¤°à¥à¤¯ à¤®à¥‡à¤‚ à¤®à¥‡à¤°à¥€ à¤®à¤¦à¤¦ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚?\n",
      "\n",
      "English Sentence: The cat is sleeping on the sofa.\n",
      "Generated Hindi Translation: , à¤à¤• gat ka sote ka à¤‡à¤¤à¤¨ ka à¤¹\n",
      "Human: à¤•à¥à¤¯à¤¾ à¤†à¤ª à¤•à¥à¤¯à¤¾ à¤¹à¥ˆà¤‚ à¤•à¤¿ à¤†à¤ª à¤‡à¤¸à¥à¤§à¤°à¤£ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆà¤‚ à¤•à¤¿ à¤†à¤ª à¤•à¥à¤¯à¤¾ à¤¹à¥ˆà¤‚ à¤•à¤¿ à¤†à¤ª à¤•à¥à¤¯à¤¾ à¤¹à¥ˆà¤‚ à¤•à¤¿ à¤†à¤ª à¤•à¥à¤¯à¤¾ à¤¹à¥ˆà¤‚ à¤•à¤¿ à¤†à¤ª à¤•à¥à¤¯à¤¾ à¤¹à¥ˆà¤‚ à¤•à¤¿ à¤†à¤ª à¤•à¥à¤¯à¤¾ à¤¹à¥ˆà¤‚ à¤•à¤¿ à¤†à¤ª à¤•à¥à¤¯à¤¾ à¤¹à¥ˆà¤‚ à¤•à¤¿ à¤†à¤ª à¤•à¥à¤¯à¤¾ à¤¹\n",
      "Reference Hindi Translation: à¤¬à¤¿à¤²à¥à¤²à¥€ à¤¸à¥‹à¤«à¥‡ à¤ªà¤° à¤¸à¥‹ à¤°à¤¹à¥€ à¤¹à¥ˆà¥¤\n",
      "\n",
      "English Sentence: I love learning new languages.\n",
      "Generated Hindi Translation: à¤®à¥ˆà¤‚ à¤¨à¤ languages ka à¤•à¥à¤¯à¤¾ à¤†à¤ª à¤®à¥ˆà¤‚ à¤ªà¤¸ à¤ªà¤¸ à¤¹à¥ˆà¤‚\n",
      "Human: à¤•à¥à¤¯à¤¾ à¤†à¤ª à¤®à¥ˆà¤‚ à¤•à¥à¤¯à¤¾ à¤†à¤ª à¤®à¥ˆà¤‚ à¤•à¥à¤¯à¤¾ à¤†à¤ª à¤®à¥ˆà¤‚ à¤¨à¤ languages ka à¤•à¥à¤¯à¤¾ à¤†à¤ª\n",
      "Reference Hindi Translation: à¤®à¥à¤à¥‡ à¤¨à¤ˆ à¤­à¤¾à¤·à¤¾à¤à¤ à¤¸à¥€à¤–à¤¨à¤¾ à¤ªà¤¸à¤‚à¤¦ à¤¹à¥ˆà¥¤\n",
      "\n",
      "English Sentence: She sells seashells by the seashore.\n",
      "Generated Hindi Translation: , à¤µà¤¹i ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka ka\n",
      "Reference Hindi Translation: à¤µà¤¹ à¤¸à¤®à¥à¤¦à¥à¤° à¤¤à¤Ÿ à¤ªà¤° à¤¸à¥€à¤ªà¤¿à¤¯à¤¾à¤ à¤¬à¥‡à¤šà¤¤à¥€ à¤¹à¥ˆà¥¤\n",
      "\n",
      "English Sentence: Technology is advancing rapidly.\n",
      "Generated Hindi Translation: \n",
      "Reference Hindi Translation: à¤ªà¥à¤°à¥Œà¤¦à¥à¤¯à¥‹à¤—à¤¿à¤•à¥€ à¤¤à¥‡à¤œà¥€ à¤¸à¥‡ à¤‰à¤¨à¥à¤¨à¤¤à¤¿ à¤•à¤° à¤°à¤¹à¥€ à¤¹à¥ˆà¥¤\n",
      "\n",
      "English Sentence: Let's meet at the cafe tomorrow.\n",
      "Generated Hindi Translation: à¥‡ à¤®à¥à¤à¥‡ à¤•à¥à¤› à¤¸à¤®à¤¯ à¤®à¥‡à¤‚ à¤•à¥à¤› à¤¸à¥à¤¥à¤¾à¤¨ à¤¹à¥‹à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¥à¤› à¤¸à¥à¤¥à¤¾à¤¨ à¤¹à¥‹à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¥à¤› à¤¸à¥à¤¥à¤¾à¤¨ à¤¹à¥‹à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¥à¤› à¤¸à¥à¤¥à¤¾à¤¨ à¤¹à¥‹à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¥à¤› à¤¸à¥à¤¥à¤¾à¤¨ à¤¹à¥‹à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¥à¤›\n",
      "Reference Hindi Translation: à¤†à¤‡à¤ à¤•à¤² à¤•à¥ˆà¤«à¤¼à¥‡ à¤®à¥‡à¤‚ à¤®à¤¿à¤²à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤\n",
      "\n",
      "English Sentence: He won the award for best actor.\n",
      "Generated Hindi Translation: à¤¨\n",
      "Reference Hindi Translation: à¤‰à¤¸à¥‡ à¤¸à¤°à¥à¤µà¤¶à¥à¤°à¥‡à¤·à¥à¤  à¤…à¤­à¤¿à¤¨à¥‡à¤¤à¤¾ à¤•à¤¾ à¤ªà¥à¤°à¤¸à¥à¤•à¤¾à¤° à¤®à¤¿à¤²à¤¾à¥¤\n",
      "\n",
      "BLEU Score: 0.12\n",
      "CHRF Score: 2.32\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from sacrebleu import corpus_bleu, corpus_chrf  # Added for evaluation\n",
    "\n",
    "# Load your specific model and tokenizer\n",
    "model_id = \"subhrokomol/Mistral-7B-Instruct-v0.3-transtokenized\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Define the prompt template as in your original code\n",
    "def create_prompt(conversation):\n",
    "    prompt = \"\"\n",
    "    for message in conversation:\n",
    "        if message[\"role\"] == \"user\":\n",
    "            prompt += f\"Human: {message['content']}\\n\"\n",
    "        elif message[\"role\"] == \"assistant\":\n",
    "            prompt += f\"Assistant: {message['content']}\\n\"\n",
    "    prompt += \"Assistant: \"\n",
    "    return prompt\n",
    "\n",
    "# Prepare a list of English sentences and their Hindi references\n",
    "english_sentences = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"How are you doing today?\",\n",
    "    \"The weather is nice outside.\",\n",
    "    \"Can you help me with this task?\",\n",
    "    \"The cat is sleeping on the sofa.\",\n",
    "    \"I love learning new languages.\",\n",
    "    \"She sells seashells by the seashore.\",\n",
    "    \"Technology is advancing rapidly.\",\n",
    "    \"Let's meet at the cafe tomorrow.\",\n",
    "    \"He won the award for best actor.\"\n",
    "]\n",
    "\n",
    "reference_hindi = [\n",
    "    \"à¤«à¥à¤°à¤¾à¤‚à¤¸ à¤•à¥€ à¤°à¤¾à¤œà¤§à¤¾à¤¨à¥€ à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?\",\n",
    "    \"à¤†à¤œ à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¹à¥ˆà¤‚?\",\n",
    "    \"à¤¬à¤¾à¤¹à¤° à¤•à¤¾ à¤®à¥Œà¤¸à¤® à¤…à¤šà¥à¤›à¤¾ à¤¹à¥ˆà¥¤\",\n",
    "    \"à¤•à¥à¤¯à¤¾ à¤†à¤ª à¤‡à¤¸ à¤•à¤¾à¤°à¥à¤¯ à¤®à¥‡à¤‚ à¤®à¥‡à¤°à¥€ à¤®à¤¦à¤¦ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚?\",\n",
    "    \"à¤¬à¤¿à¤²à¥à¤²à¥€ à¤¸à¥‹à¤«à¥‡ à¤ªà¤° à¤¸à¥‹ à¤°à¤¹à¥€ à¤¹à¥ˆà¥¤\",\n",
    "    \"à¤®à¥à¤à¥‡ à¤¨à¤ˆ à¤­à¤¾à¤·à¤¾à¤à¤ à¤¸à¥€à¤–à¤¨à¤¾ à¤ªà¤¸à¤‚à¤¦ à¤¹à¥ˆà¥¤\",\n",
    "    \"à¤µà¤¹ à¤¸à¤®à¥à¤¦à¥à¤° à¤¤à¤Ÿ à¤ªà¤° à¤¸à¥€à¤ªà¤¿à¤¯à¤¾à¤ à¤¬à¥‡à¤šà¤¤à¥€ à¤¹à¥ˆà¥¤\",\n",
    "    \"à¤ªà¥à¤°à¥Œà¤¦à¥à¤¯à¥‹à¤—à¤¿à¤•à¥€ à¤¤à¥‡à¤œà¥€ à¤¸à¥‡ à¤‰à¤¨à¥à¤¨à¤¤à¤¿ à¤•à¤° à¤°à¤¹à¥€ à¤¹à¥ˆà¥¤\",\n",
    "    \"à¤†à¤‡à¤ à¤•à¤² à¤•à¥ˆà¤«à¤¼à¥‡ à¤®à¥‡à¤‚ à¤®à¤¿à¤²à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤\",\n",
    "    \"à¤‰à¤¸à¥‡ à¤¸à¤°à¥à¤µà¤¶à¥à¤°à¥‡à¤·à¥à¤  à¤…à¤­à¤¿à¤¨à¥‡à¤¤à¤¾ à¤•à¤¾ à¤ªà¥à¤°à¤¸à¥à¤•à¤¾à¤° à¤®à¤¿à¤²à¤¾à¥¤\"\n",
    "]\n",
    "\n",
    "# List to store generated translations\n",
    "generated_translations = []\n",
    "\n",
    "# Loop over each English sentence to generate Hindi translations\n",
    "for sentence in english_sentences:\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Please translate the following sentence to Hindi: '{sentence}'\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Create the prompt using the existing function\n",
    "    prompt = create_prompt(conversation)\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Remove token_type_ids from inputs if present\n",
    "    inputs.pop('token_type_ids', None)\n",
    "\n",
    "    # Generate the response\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract the translation from the response\n",
    "    translation = response.split(\"Assistant:\")[-1].strip()\n",
    "\n",
    "    # Append the translation to the list\n",
    "    generated_translations.append(translation)\n",
    "\n",
    "# Evaluate translations using BLEU and CHRF\n",
    "bleu_score = corpus_bleu(generated_translations, [reference_hindi]).score\n",
    "chrf_score = corpus_chrf(generated_translations, [reference_hindi]).score\n",
    "\n",
    "# Display the results\n",
    "print(\"Generated Translations and Reference Comparisons:\\n\")\n",
    "for eng, gen, ref in zip(english_sentences, generated_translations, reference_hindi):\n",
    "    print(f\"English Sentence: {eng}\")\n",
    "    print(f\"Generated Hindi Translation: {gen}\")\n",
    "    print(f\"Reference Hindi Translation: {ref}\\n\")\n",
    "\n",
    "print(f\"BLEU Score: {bleu_score:.2f}\")\n",
    "print(f\"CHRF Score: {chrf_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 3/3 [01:14<00:00, 24.93s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.93s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "import torch\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> You are an AI assistant specialized in explaining complex scientific concepts in simple terms.\n",
      "Explain quantum entanglement to a 10-year-old.\n",
      "Sure, let's imagine you have two magic dice that always show the same number when rolled at exactly the same time—no matter how far apart they are! This strange connection between them is called \"quantum entangling.\" It means their states become linked and can affect each other instantly no matter what distance separates them (even if it’s across galaxies). But here comes the tricky part: this doesn't work with regular objects like your toys or me; only tiny particles\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"You are an AI assistant specialized in explaining complex scientific concepts in simple terms.\n",
    "Explain quantum entanglement to a 10-year-old.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Get the number of tokens in the prompt\n",
    "prompt_length = inputs.input_ids.shape[1]\n",
    "\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_length=prompt_length + 100,  # Prompt length plus 100 new tokens\n",
    "    min_new_tokens=1,  # Ensure at least one new token is generated\n",
    "    num_beams=1,\n",
    "    no_repeat_ngram_size=3,\n",
    "    repetition_penalty=1.5,\n",
    "    temperature=0.7,\n",
    "    top_p=0.92,\n",
    "    do_sample=True,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "# Decode only the new tokens\n",
    "model_response = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "# model_response = tokenizer.decode(output[0][prompt_length], skip_special_tokens=False) #gives only the output\n",
    "print(model_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing json \n",
    "import json\n",
    "\n",
    "with open('Hindi_Benchmark_data.json', 'r', encoding='utf-8') as f:\n",
    "    dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to get model response\n",
    "def get_model_response(question):\n",
    "    prompt = f\"<s>[INST] {question} [/INST]\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    prompt_length = inputs.input_ids.shape[1]\n",
    "    \n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=prompt_length + 100,\n",
    "        num_beams=1,\n",
    "        temperature=0.7,\n",
    "        top_p=0.92,\n",
    "        do_sample=True,\n",
    "        use_cache=True\n",
    "    )\n",
    "    \n",
    "    model_response = tokenizer.decode(output[0][prompt_length:], skip_special_tokens=False)\n",
    "    return model_response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def calculate_perplexity(question, answer):\n",
    "    inputs = tokenizer(question + \" \" + answer, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    return torch.exp(outputs.loss).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:36<00:00, 12.29s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: अकबर और बीरबल एक दिन बगीचे में टहल रहे थे। अचानक अकबर ने बीरबल से पूछा, 'क्या तुम जान सकते हो कि आज मेरे दिमाग में क्या चल रहा है?' बीरबल ने तुरंत उत्तर दिया, 'हां महाराज, आप इस समय बगीचे की सुंदरता का आनंद ले रहे हैं।' अकबर बीरबल की बुद्धिमानी से बहुत प्रभावित हुए। प्रश्न: अकबर और बीरबल किस जगह टहल रहे थे?\n",
      "Reference Answer: बगीचे में\n",
      "Model Answer: अकबар और बीरबल बगीचे में टहल रहे थे।\n",
      "Perplexity: 2.6256015300750732\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: तेनालीराम के पास एक अनोखा तोता था जो लोगों की नकल बहुत अच्छी तरह से करता था। एक बार राजा ने तेनालीराम से पूछा, 'क्या तुम्हारा तोता मेरा नाम ले सकता है?' तेनालीराम ने तोते से कहा, 'राजा का नाम लो।' तोते ने तुरंत कहा, 'राजा कृष्णदेव राय।' राजा यह सुनकर बहुत खुश हुए। प्रश्न: तेनालीराम का तोता क्या कर सकता था?\n",
      "Reference Answer: लोगों की नकल\n",
      "Model Answer: Tenali Ramakrishna's unique parrot could mimic people's voices very well.\n",
      "\n",
      "Answer: Tenali Ramakrishna's parrot could mimic people's voices.\n",
      "Perplexity: 2.556901693344116\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दिए गए विकल्पों में से किस संख्या श्रृंखला को पूरा करेगा? 2, 6, 12, 20, ___\n",
      "Reference Answer: 30\n",
      "Model Answer: 40 श्रृंखला को पूरा करता है 2, 6, 12, 20, 40 का सेतु. यह एक और एक और एक अर्थात् 2, 6, 12, 20,\n",
      "Perplexity: 2.719683885574341\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: एक किसान के पास एक बकरी, एक गाजर और एक शेर था। उसे नदी पार करनी थी लेकिन नाव में एक बार में केवल एक ही चीज ले जा सकता था। यदि वह बकरी और गाजर को साथ छोड़ता है, तो बकरी गाजर खा जाएगी। यदि वह शेर और बकरी को साथ छोड़ता है, तो शेर बकरी खा जाएगा। किसान कैसे सभी को सुरक्षित नदी पार कराएगा?\n",
      "Reference Answer: पहले बकरी को पार ले जाए, फिर गाजर को ले जाए और बकरी को वापस लाए। अब शेर को पार ले जाए और फिर बकरी को वापस लाए। अंत में, बकरी को पुनः पार ले जाए।\n",
      "Model Answer: उस किसान ने बकरी और गाजर को बिना अलग ले कर दो नदी के दो ओर से चला दिया। फिर बकरी और गाजर को अ\n",
      "Perplexity: 2.3577167987823486\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: अकबर ने बीरबल से पूछा, 'बीरबल, तुम्हारे पास कितना ज्ञान है?' बीरबल ने उत्तर दिया, 'महाराज, ज्ञान की कोई सीमा नहीं होती, यह अनंत है।' प्रश्न: बीरबल का ज्ञान कैसा था?\n",
      "Reference Answer: अनंत\n",
      "Model Answer: बीरबल का ज्ञान अनंत था, क्योकि उन्होंने कहा था कि ज्ञान की कोई सीमा नहीं होती है।\n",
      "Perplexity: 2.1273910999298096\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: एक बार अकबर ने बीरबल से पूछा कि सबसे बड़ा सत्य क्या है। बीरबल ने उत्तर दिया, 'महाराज, समय ही सबसे बड़ा सत्य है, क्योंकि यह किसी के लिए नहीं रुकता।' प्रश्न: बीरबल के अनुसार सबसे बड़ा सत्य क्या है?\n",
      "Reference Answer: समय\n",
      "Model Answer: बीरबल के अनुसार, समय सबसे बड़ा सत्य है, क्योंकि यह किसी के लिए नहीं रुकता। इस वाक्य के अर्थ से, समय\n",
      "Perplexity: 2.132641553878784\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दिए गए विकल्पों में से सही उत्तर चुनें: अकबर का जन्म कहाँ हुआ था? (क) आगरा (ख) दिल्ली (ग) काबुल (घ) अजमेर\n",
      "Reference Answer: अजमेर\n",
      "Model Answer: अकबर का जन्म काबुल (घ) में हुआ था.\n",
      "Perplexity: 2.848371982574463\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: तेनालीराम ने एक बार एक प्रतियोगिता में भाग लिया जहाँ उन्हें अपनी बुद्धिमानी का प्रदर्शन करना था। उसने एक पहेली सुलझाई जिससे सभी प्रभावित हुए। प्रश्न: तेनालीराम ने किस प्रतियोगिता में भाग लिया?\n",
      "Reference Answer: बुद्धिमानी की प्रतियोगिता\n",
      "Model Answer: Tenali Ram ने कहाँ प्रतियोगिता में भाग लिया उसका उत्तर नहीं दिया गया है। यह सामग्री के अधिकारियों के लिए\n",
      "Perplexity: 2.463453531265259\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: एक बार बीरबल को साबित करना था कि उनके पास अनोखी बुद्धिमानी है। उन्होंने एक मुश्किल समस्या का हल निकालकर सभी को चौंका दिया। प्रश्न: बीरबल ने क्या साबित किया?\n",
      "Reference Answer: कि उनके पास अनोखी बुद्धिमानी है\n",
      "Model Answer: बीरबल ने साबित किया कि वह अनोखी बुद्धिमान है। लेकिन प्रश्न का हल देने के लिए कुछ अधिक जावाब चाहिए\n",
      "Perplexity: 2.3873822689056396\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दिए गए विकल्पों में से सही उत्तर चुनें: तेनालीराम का पूरा नाम क्या था? (क) तेनाली रामकृष्ण (ख) तेनाली रामू (ग) तेनाली रामायन (घ) तेनाली रामस्वामी\n",
      "Reference Answer: तेनाली रामकृष्ण\n",
      "Model Answer: तेनाली रामकृष्ण (क) था।\n",
      "Perplexity: 3.0049307346343994\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: अकबर ने बीरबल से पूछा, 'बीरबल, यदि तुम्हारे पास तीन सेब हैं और तुम्हें मुझे एक देना है, तो तुम्हारे पास कितने सेब बचेंगे?' बीरबल ने उत्तर दिया, 'महाराज, मेरे पास तीन सेब थे, लेकिन आपने कहा कि मुझे एक देना है, इसका मतलब है कि आपके पास एक और मेरे पास दो बचेंगे।' प्रश्न: अकबर ने बीरबल से क्या पूछा?\n",
      "Reference Answer: यदि तुम्हारे पास तीन सेब हैं और तुम्हें मुझे एक देना है, तो तुम्हारे पास कितने सेब बचेंगे?\n",
      "Model Answer: अकबर ने बीरबल से 'तुम्हारे पास तीन सेब थे, लेकिन तुम्हें मुझे एक देना है, तो तुम्हारे पास कितने से\n",
      "Perplexity: 1.9435442686080933\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: एक दिन राजा कृष्णदेव राय ने तेनालीराम से कहा कि वे एक ऐसी कविता लिखें जिसमें केवल तीन पंक्तियाँ हों, लेकिन उसमें गहरी बुद्धिमानी हो। तेनालीराम ने तुरंत एक कविता बनाई और राजा को सुनाई। राजा बहुत खुश हुए। प्रश्न: राजा ने तेनालीराम से क्या लिखने को कहा?\n",
      "Reference Answer: तीन पंक्तियों वाली गहरी बुद्धिमानी की कविता\n",
      "Model Answer: प्रतिज्ञा कि नीचे दिए गए कविताओं में ही जवाब है:\n",
      "\n",
      "1.\n",
      "\n",
      "वह तरह है जिसका स्थान मेरे दिल में है,\n",
      "वह\n",
      "Perplexity: 2.5603272914886475\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: अकबर ने बीरबल से पूछा कि संसार की सबसे बड़ी शक्ति क्या है। बीरबल ने उत्तर दिया, 'महाराज, प्रेम सबसे बड़ी शक्ति है क्योंकि यह सभी को जोड़ सकता है।' प्रश्न: बीरबल के अनुसार संसार की सबसे बड़ी शक्ति क्या है?\n",
      "Reference Answer: प्रेम\n",
      "Model Answer: अनुत्तर: कृपया मुझे से एक साथ फर्मेट को दिजिये, कि मैं अपने बोत को फॉर्मेट के अनुसार सक्षम रख\n",
      "Perplexity: 2.459002733230591\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: तेनालीराम एक दिन बाजार में घूम रहे थे, जब उन्होंने देखा कि एक आदमी झूठ बोल रहा है। तेनालीराम ने उस आदमी से कहा, 'सत्य बोलना सबसे बड़ी नैतिकता है।' आदमी को अपनी गलती का एहसास हुआ और उसने तेनालीराम से माफी माँगी। प्रश्न: तेनालीराम ने आदमी से क्या कहा?\n",
      "Reference Answer: सत्य बोलना सबसे बड़ी नैतिकता है\n",
      "Model Answer: Tenaaliram ne us adam se kaha, \"Satya bolna sabse baraati naitikta hai.\" (Translation: Tenaaliram said, \"Telling the truth is the greatest virtue.\")\n",
      "Perplexity: 2.3191986083984375\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: अकबर ने बीरबल से कहा कि वे एक ऐसा तरीका बताएं जिससे सभी लोग खुश हो सकें। बीरबल ने उत्तर दिया, 'महाराज, सबको न्याय और समानता दीजिए, तभी सभी लोग खुश हो सकते हैं।' प्रश्न: बीरबल ने अकबर को क्या तरीका बताया?\n",
      "Reference Answer: सबको न्याय और समानता देना\n",
      "Model Answer: बीरबल ने अकबर को उस तरीका बताया कि सभी लोग खुश हो सकें, जिससे महाराज (विदेश और सरकार) सबको न्याय �\n",
      "Perplexity: 2.400193929672241\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: एक बार तेनालीराम ने अपने दोस्त को एक अनोखी घड़ी दी जो हर घंटे अलग-अलग रंग में बदलती थी। उसके दोस्त को यह घड़ी बहुत पसंद आई और उसने तेनालीराम को धन्यवाद दिया। प्रश्न: तेनालीराम ने अपने दोस्त को क्या दिया?\n",
      "Reference Answer: अनोखी घड़ी\n",
      "Model Answer: Tenaliraman ने अपने दोस्त को उस अनोखी घड़ी दिया, जो हर घंटे अलग-अलग रंग में बदलती थी।\n",
      "Perplexity: 2.236809015274048\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: अकबर ने बीरबल से कहा कि वे एक ऐसा प्रश्न पूछें जिसका उत्तर कोई न दे सके। बीरबल ने पूछा, 'महाराज, क्या आप बता सकते हैं कि इस समय आपके दिमाग में क्या चल रहा है?' अकबर मुस्कुरा दिए और बोले, 'बीरबल, तुमने मुझे सोचने पर मजबूर कर दिया।' प्रश्न: बीरबल ने अकबर से क्या पूछा?\n",
      "Reference Answer: क्या आप बता सकते हैं कि इस समय आपके दिमाग में क्या चल रहा है?\n",
      "Model Answer: बीरबल ने अकबर से पूछा कि 'क्या आपके दिमाग में क्या चल रहा है?' है.\n",
      "Perplexity: 2.343766927719116\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: तेनालीराम ने एक बार अपने राजा के लिए एक विशेष भोज का आयोजन किया जिसमें उन्होंने हर प्रकार के स्वादिष्ट व्यंजन परोसे। राजा ने तेनालीराम की प्रशंसा की और कहा कि यह उनके जीवन का सबसे अच्छा भोज था। प्रश्न: तेनालीराम ने राजा के लिए क्या आयोजन किया?\n",
      "Reference Answer: विशेष भोज\n",
      "Model Answer: Tenaliraman ने राजा के लिए एक विशेष भोज का आयोजन किया जिसमें उन्होंने हर प्रकार के स्वादिष्ट व्यंजन परोसे (उ\n",
      "Perplexity: 2.1416046619415283\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: अकबर ने बीरबल से पूछा कि सबसे कीमती वस्तु क्या है। बीरबल ने उत्तर दिया, 'महाराज, सबसे कीमती वस्तु समय है क्योंकि यह कभी वापस नहीं आता।' प्रश्न: बीरबल के अनुसार सबसे कीमती वस्तु क्या है?\n",
      "Reference Answer: समय\n",
      "Model Answer: बीरबल के अनुसार, सबसे कीमती वस्तु समय है, क्योंकि यह कभी वापस नहीं आता।\n",
      "Perplexity: 2.1306121349334717\n",
      "---\n",
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: एक दिन तेनालीराम ने देखा कि एक गरीब आदमी भूखा है। उन्होंने उस आदमी को भोजन कराया और उसे अपनी कुछ धनराशि भी दी। वह आदमी तेनालीराम का आभारी हो गया और उसने तेनालीराम को आशीर्वाद दिया। प्रश्न: तेनालीराम ने उस गरीब आदमी के लिए क्या किया?\n",
      "Reference Answer: भोजन कराया और धनराशि दी\n",
      "Model Answer: Tenali Ram ने उस गरीब आदमी को भोजन दिया और उसे अपनी कुछ धनराशि भी दी।\n",
      "Perplexity: 2.283085584640503\n",
      "---\n",
      "Overall Perplexity Score: 2.3886796119444584\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load Mistral model and tokenizer\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "def get_model_response(question):\n",
    "    prompt = f\"<s>[INST] {question} [/INST]\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    prompt_length = inputs.input_ids.shape[1]\n",
    "    \n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=prompt_length + 100,\n",
    "        num_beams=1,\n",
    "        temperature=0.7,\n",
    "        top_p=0.92,\n",
    "        do_sample=True,\n",
    "        use_cache=True\n",
    "    )\n",
    "    \n",
    "    model_response = tokenizer.decode(output[0][prompt_length:], skip_special_tokens=True)\n",
    "    return model_response.strip()\n",
    "\n",
    "def calculate_perplexity(question, answer):\n",
    "    inputs = tokenizer(question + \" \" + answer, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    return torch.exp(outputs.loss).item()\n",
    "\n",
    "# Load the dataset\n",
    "with open('Hindi_Benchmark_data.json', 'r', encoding='utf-8') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "perplexities = []\n",
    "\n",
    "# Process only the first 20 questions\n",
    "for item in dataset[:20]:\n",
    "    question = item[\"Question\"]\n",
    "    reference_answer = item[\"Answer\"]\n",
    "    \n",
    "    model_answer = get_model_response(question)\n",
    "    \n",
    "    perplexity = calculate_perplexity(question, model_answer)\n",
    "    perplexities.append(perplexity)\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Reference Answer: {reference_answer}\")\n",
    "    print(f\"Model Answer: {model_answer}\")\n",
    "    print(f\"Perplexity: {perplexity}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# Calculate the overall perplexity score\n",
    "overall_perplexity = np.exp(np.mean(np.log(perplexities)))\n",
    "\n",
    "print(f\"Overall Perplexity Score: {overall_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying with LORA model transtokenized + Finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: BNB_CUDA_VERSION=118 environment variable detected; loading libbitsandbytes_cuda118.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"<s> Below is a conversation between a human and an AI assistant. Write a response that appropriately continues the conversation.\\n\\n### Conversation:\\nWhat is a famous tall tower in Paris?\\n\\n### Assistant:\\nThe Eiffel Tower is a famous tall tower in Paris. It's one of the most recognizable landmarks in the world.\\n\\n### Human:\\nHow tall is it?\\n\\n### Assistant:\\nThe Eiffel Tower stands at approximately 330 meters (1,08\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "#code to load the LORA model and run it.\n",
    "\n",
    "# Load the model\n",
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"subhrokomol/Mistral-7B-Instruct-v0.3-transtokenized\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# alpaca_prompt = \"\"\"Below is a conversation between a human and an AI assistant. Write a response that appropriately continues the conversation.\n",
    "\n",
    "# ### Conversation:\n",
    "# {}\n",
    "\n",
    "# ### Assistant:\"\"\"\n",
    "\n",
    "# inputs = tokenizer(\n",
    "# [\n",
    "#     alpaca_prompt.format(\n",
    "#         \"What is a famous tall tower in Paris?\", # instruction\n",
    "#         \"\", # input\n",
    "#         \"\", # output - leave this blank for generation!\n",
    "#     ),\n",
    "# ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "# outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "# tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_response(question):\n",
    "    # Format the prompt\n",
    "    prompt = f\"<s>[INST] {question} [/INST]\"\n",
    "    \n",
    "    # Tokenize and generate\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
    "    \n",
    "    # Decode and clean up the response\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the model's response\n",
    "    response = full_response.split(\"[INST]\")[-1].split(\"[/INST]\")[-1].strip()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दिए गए विकल्पों में से किस संख्या श्रृंखला को पूरा करेगा? 2, 6, 12, 20, ___\n",
      "Answer: नीचे दिए गए विकल्पों में से किस संख्या श्रृंखला को पूरा करेगा? 2, 6, 12, 20, ___  30 श्रृंखला को पूरा करेगा। यह सरल श्रृंखला है, जisme सबसे पहले\n"
     ]
    }
   ],
   "source": [
    "question = \"नीचे दिए गए विकल्पों में से किस संख्या श्रृंखला को पूरा करेगा? 2, 6, 12, 20, ___\"\n",
    "answer = get_model_response(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: अकबर और बीरबल एक दिन बगीचे में टहल रहे थे। अचानक अकबर ने बीरबल से पूछा, 'क्या तुम जान सकते हो कि आज मेरे दिमाग में क्या चल रहा है?' बीरबल ने तुरंत उत्तर दिया, 'हां महाराज, आप इस समय बगीचे की सुंदरता का आनंद ले रहे हैं।' अकबर बीरबल की बुद्धिमानी से बहुत प्रभावित हुए। प्रश्न: अकबर और बीरबल किस जगह टहल रहे थे?\n",
      "Reference Answer: बगीचे में\n",
      "Model Answer: अकबर और बीरबल किस जगह टहल रहे थे?  अकबар और बीरबल बगीचे में थे।\n",
      "Perplexity: 2.5458455085754395\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: तेनालीराम के पास एक अनोखा तोता था जो लोगों की नकल बहुत अच्छी तरह से करता था। एक बार राजा ने तेनालीराम से पूछा, 'क्या तुम्हारा तोता मेरा नाम ले सकता है?' तेनालीराम ने तोते से कहा, 'राजा का नाम लो।' तोते ने तुरंत कहा, 'राजा कृष्णदेव राय।' राजा यह सुनकर बहुत खुश हुए। प्रश्न: तेनालीराम का तोता क्या कर सकता था?\n",
      "Reference Answer: लोगों की नकल\n",
      "Model Answer: तेनालीराम का तोता क्या कर सकता था?  Tenali Raman का तोता सोना था, जो लोगों की नकल बहुत अच्छी तरह से करता था। इस प्रकार कि जब राजा कृष्णदेव र\n",
      "Perplexity: 2.334564685821533\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दिए गए विकल्पों में से किस संख्या श्रृंखला को पूरा करेगा? 2, 6, 12, 20, ___\n",
      "Reference Answer: 30\n",
      "Model Answer: नीचे दिए गए विकल्पों में से किस संख्या श्रृंखला को पूरा करेगा? 2, 6, 12, 20, ___  40 श्रृंखला को पूरा करता है 2, 6, 12, 20, 40 के साथ। यह अर्थात श्रृंखला के साथ पहले 3 श्रृंखल\n",
      "Perplexity: 2.0721652507781982\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: एक किसान के पास एक बकरी, एक गाजर और एक शेर था। उसे नदी पार करनी थी लेकिन नाव में एक बार में केवल एक ही चीज ले जा सकता था। यदि वह बकरी और गाजर को साथ छोड़ता है, तो बकरी गाजर खा जाएगी। यदि वह शेर और बकरी को साथ छोड़ता है, तो शेर बकरी खा जाएगा। किसान कैसे सभी को सुरक्षित नदी पार कराएगा?\n",
      "Reference Answer: पहले बकरी को पार ले जाए, फिर गाजर को ले जाए और बकरी को वापस लाए। अब शेर को पार ले जाए और फिर बकरी को वापस लाए। अंत में, बकरी को पुनः पार ले जाए।\n",
      "Model Answer: नीचे दी गई कहानी को पढ़कर उत्तर दें: एक किसान के पास एक बकरी, एक गाजर और एक शेर था। उसे नदी पार करनी थी लेकिन नाव में एक बार में केवल एक ही चीज ले जा सकता था। यदि वह बकरी और गाजर को साथ छोड़ता है, तो बकरी गाजर खा जाएगी। यदि वह शेर और बकरी को साथ छोड़ता है, तो शेर बकरी खा जाएगा। किसान कैसे सभी को सुरक्षित नदी पार कराएगा?  किसान ने बकरी और गाजर को एक दूसरे से दूर रखकर ही नदी पार कर लिया था, और शेर को बाँके पर रखा थ\n",
      "Perplexity: 1.6607437133789062\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: अकबर ने बीरबल से पूछा, 'बीरबल, तुम्हारे पास कितना ज्ञान है?' बीरबल ने उत्तर दिया, 'महाराज, ज्ञान की कोई सीमा नहीं होती, यह अनंत है।' प्रश्न: बीरबल का ज्ञान कैसा था?\n",
      "Reference Answer: अनंत\n",
      "Model Answer: बीरबल का ज्ञान कैसा था?  बीरबल का उत्तर दिया गया था कि 'ज्ञान की कोई सीमा नहीं होती, यह अनंत है'. अतृप्त रहकर यह पता चला क\n",
      "Perplexity: 2.174736261367798\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: एक बार अकबर ने बीरबल से पूछा कि सबसे बड़ा सत्य क्या है। बीरबल ने उत्तर दिया, 'महाराज, समय ही सबसे बड़ा सत्य है, क्योंकि यह किसी के लिए नहीं रुकता।' प्रश्न: बीरबल के अनुसार सबसे बड़ा सत्य क्या है?\n",
      "Reference Answer: समय\n",
      "Model Answer: बीरबल के अनुसार सबसे बड़ा सत्य क्या है?  बीरबल के अनुसार सबसे बड़ा सत्य है समय ही है, क्योंकि समय किसी के लिए नहीं रुकता।\n",
      "Perplexity: 2.073301315307617\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दिए गए विकल्पों में से सही उत्तर चुनें: अकबर का जन्म कहाँ हुआ था? (क) आगरा (ख) दिल्ली (ग) काबुल (घ) अजमेर\n",
      "Reference Answer: अजमेर\n",
      "Model Answer: नीचे दिए गए विकल्पों में से सही उत्तर चुनें: अकबर का जन्म कहाँ हुआ था? (क) आगरा (ख) दिल्ली (ग) काबुल (घ) अजमेर  अकबर का जन्म काबुल (घ) में हुआ था।\n",
      "Perplexity: 1.9295587539672852\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: तेनालीराम ने एक बार एक प्रतियोगिता में भाग लिया जहाँ उन्हें अपनी बुद्धिमानी का प्रदर्शन करना था। उसने एक पहेली सुलझाई जिससे सभी प्रभावित हुए। प्रश्न: तेनालीराम ने किस प्रतियोगिता में भाग लिया?\n",
      "Reference Answer: बुद्धिमानी की प्रतियोगिता\n",
      "Model Answer: तेनालीराम ने किस प्रतियोगिता में भाग लिया?  उस कहानी में Tenali Ram ने कहा था कि उन्हें कोई प्रतियोगिता में भाग लिया था, लेकिन मैं उस प्रतियोगिता का न\n",
      "Perplexity: 2.3090243339538574\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: एक बार बीरबल को साबित करना था कि उनके पास अनोखी बुद्धिमानी है। उन्होंने एक मुश्किल समस्या का हल निकालकर सभी को चौंका दिया। प्रश्न: बीरबल ने क्या साबित किया?\n",
      "Reference Answer: कि उनके पास अनोखी बुद्धिमानी है\n",
      "Model Answer: बीरबल ने क्या साबित किया?  बीरबल ने किया था कि वह अनोखी बुद्धिमान है। उन्होंने एक मुश्किल समस्या का हल निकालकर सभी को चौंका\n",
      "Perplexity: 2.237152338027954\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दिए गए विकल्पों में से सही उत्तर चुनें: तेनालीराम का पूरा नाम क्या था? (क) तेनाली रामकृष्ण (ख) तेनाली रामू (ग) तेनाली रामायन (घ) तेनाली रामस्वामी\n",
      "Reference Answer: तेनाली रामकृष्ण\n",
      "Model Answer: नीचे दिए गए विकल्पों में से सही उत्तर चुनें: तेनालीराम का पूरा नाम क्या था? (क) तेनाली रामकृष्ण (ख) तेनाली रामू (ग) तेनाली रामायन (घ) तेनाली रामस्वामी  सही उत्तर है (ख) तेनाली रामू (Tenali Ramakrishna) है। Tenali Ramakrishna का पूरा नाम था। वह एक औराध है जो 16 веक के अंदर हुए\n",
      "Perplexity: 1.8685845136642456\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: अकबर ने बीरबल से पूछा, 'बीरबल, यदि तुम्हारे पास तीन सेब हैं और तुम्हें मुझे एक देना है, तो तुम्हारे पास कितने सेब बचेंगे?' बीरबल ने उत्तर दिया, 'महाराज, मेरे पास तीन सेब थे, लेकिन आपने कहा कि मुझे एक देना है, इसका मतलब है कि आपके पास एक और मेरे पास दो बचेंगे।' प्रश्न: अकबर ने बीरबल से क्या पूछा?\n",
      "Reference Answer: यदि तुम्हारे पास तीन सेब हैं और तुम्हें मुझे एक देना है, तो तुम्हारे पास कितने सेब बचेंगे?\n",
      "Model Answer: अकबर ने बीरबल से क्या पूछा?  Akbar ne Birbal se poochha, 'Birbal, agar tumhare paas teen sev bhi hai aur tumhein main ek dena hai, to tumhare paas kitne sev bachenge?' (Akbar asked Birbal, 'If you have three servants and I give one to you, how many servants will you be left with?')\n",
      "Perplexity: 2.132237434387207\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: एक दिन राजा कृष्णदेव राय ने तेनालीराम से कहा कि वे एक ऐसी कविता लिखें जिसमें केवल तीन पंक्तियाँ हों, लेकिन उसमें गहरी बुद्धिमानी हो। तेनालीराम ने तुरंत एक कविता बनाई और राजा को सुनाई। राजा बहुत खुश हुए। प्रश्न: राजा ने तेनालीराम से क्या लिखने को कहा?\n",
      "Reference Answer: तीन पंक्तियों वाली गहरी बुद्धिमानी की कविता\n",
      "Model Answer: राजा ने तेनालीराम से कहा कि वे एक ऐसी कवि\n",
      "Perplexity: 2.437075614929199\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: अकबर ने बीरबल से पूछा कि संसार की सबसे बड़ी शक्ति क्या है। बीरबल ने उत्तर दिया, 'महाराज, प्रेम सबसे बड़ी शक्ति है क्योंकि यह सभी को जोड़ सकता है।' प्रश्न: बीरबल के अनुसार संसार की सबसे बड़ी शक्ति क्या है?\n",
      "Reference Answer: प्रेम\n",
      "Model Answer: बीरबल के अनुसार संसार की सबसे बड़ी शक्ति क्या है?  अनुत्तर: अनुसार बीरबल के, संसार की सबसे बड़ी शक्ति प्रेम है, क्योंकि यह सभी को जोड़ सकता है।\n",
      "Perplexity: 1.9074651002883911\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: तेनालीराम एक दिन बाजार में घूम रहे थे, जब उन्होंने देखा कि एक आदमी झूठ बोल रहा है। तेनालीराम ने उस आदमी से कहा, 'सत्य बोलना सबसे बड़ी नैतिकता है।' आदमी को अपनी गलती का एहसास हुआ और उसने तेनालीराम से माफी माँगी। प्रश्न: तेनालीराम ने आदमी से क्या कहा?\n",
      "Reference Answer: सत्य बोलना सबसे बड़ी नैतिकता है\n",
      "Model Answer: तेनालीराम ने आदमी से क्या कहा?  Tenaliraman ne usse kahaa, \"सत्य बोलना सबसे बड़ी नैतिकता है, अतृप्त रहिये।\" (Tenaliraman told him, \"Telling the truth is the greatest virtue, be content.\")\n",
      "Perplexity: 2.278646945953369\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: अकबर ने बीरबल से कहा कि वे एक ऐसा तरीका बताएं जिससे सभी लोग खुश हो सकें। बीरबल ने उत्तर दिया, 'महाराज, सबको न्याय और समानता दीजिए, तभी सभी लोग खुश हो सकते हैं।' प्रश्न: बीरबल ने अकबर को क्या तरीका बताया?\n",
      "Reference Answer: सबको न्याय और समानता देना\n",
      "Model Answer: बीरबल ने अकबर को क्या तरीका बताया?  बीरबल ने अकबर को उनके सबको न्याय और समानता देने के तरीके बताया था, जisse सभी लोग खुश हो सकेंगे। इस त\n",
      "Perplexity: 2.1649436950683594\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: एक बार तेनालीराम ने अपने दोस्त को एक अनोखी घड़ी दी जो हर घंटे अलग-अलग रंग में बदलती थी। उसके दोस्त को यह घड़ी बहुत पसंद आई और उसने तेनालीराम को धन्यवाद दिया। प्रश्न: तेनालीराम ने अपने दोस्त को क्या दिया?\n",
      "Reference Answer: अनोखी घड़ी\n",
      "Model Answer: तेनालीराम ने अपने दोस्त को क्या दिया?  जवाब: तेनालीराम ने अपने दोस्त को एक अनोखी घड़ी दी जो हर घंटे अलग-अलग रंग में बदलती थी।\n",
      "Perplexity: 2.0129036903381348\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: अकबर ने बीरबल से कहा कि वे एक ऐसा प्रश्न पूछें जिसका उत्तर कोई न दे सके। बीरबल ने पूछा, 'महाराज, क्या आप बता सकते हैं कि इस समय आपके दिमाग में क्या चल रहा है?' अकबर मुस्कुरा दिए और बोले, 'बीरबल, तुमने मुझे सोचने पर मजबूर कर दिया।' प्रश्न: बीरबल ने अकबर से क्या पूछा?\n",
      "Reference Answer: क्या आप बता सकते हैं कि इस समय आपके दिमाग में क्या चल रहा है?\n",
      "Model Answer: बीरबल ने अकबर से क्या पूछा?  बीरबल ने अकबर से 'क्या आपके दिमाग में क्या चल रहा है?' पूछा.\n",
      "Perplexity: 2.2544219493865967\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: तेनालीराम ने एक बार अपने राजा के लिए एक विशेष भोज का आयोजन किया जिसमें उन्होंने हर प्रकार के स्वादिष्ट व्यंजन परोसे। राजा ने तेनालीराम की प्रशंसा की और कहा कि यह उनके जीवन का सबसे अच्छा भोज था। प्रश्न: तेनालीराम ने राजा के लिए क्या आयोजन किया?\n",
      "Reference Answer: विशेष भोज\n",
      "Model Answer: तेनालीराम ने राजा के लिए क्या आयोजन किया?  Tenaliraman ने राजा के लिए एक विशेष भोज का आयोजन किया जिसमें उन्होंने हर प्रकार के स्वादिष्ट व्यंजन परोसे। (\n",
      "Perplexity: 2.0333378314971924\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: अकबर ने बीरबल से पूछा कि सबसे कीमती वस्तु क्या है। बीरबल ने उत्तर दिया, 'महाराज, सबसे कीमती वस्तु समय है क्योंकि यह कभी वापस नहीं आता।' प्रश्न: बीरबल के अनुसार सबसे कीमती वस्तु क्या है?\n",
      "Reference Answer: समय\n",
      "Model Answer: बीरबल के अनुसार सबसे कीमती वस्तु क्या है?  अनुत्तर: बीरबल के अनुसार, सबसे कीमती वस्तु समय है, क्योंकि यह कभी वापस नहीं आता।\n",
      "Perplexity: 2.029245376586914\n",
      "---\n",
      "Question: नीचे दी गई कहानी को पढ़कर उत्तर दें: एक दिन तेनालीराम ने देखा कि एक गरीब आदमी भूखा है। उन्होंने उस आदमी को भोजन कराया और उसे अपनी कुछ धनराशि भी दी। वह आदमी तेनालीराम का आभारी हो गया और उसने तेनालीराम को आशीर्वाद दिया। प्रश्न: तेनालीराम ने उस गरीब आदमी के लिए क्या किया?\n",
      "Reference Answer: भोजन कराया और धनराशि दी\n",
      "Model Answer: तेनालीराम ने उस गरीब आदमी के लिए क्या किया?  तेनालीराम ने गरीब आदमी को भोजन देखा और उसके लिए अपनी कुछ धनराशि भी दी।\n",
      "Perplexity: 2.1496379375457764\n",
      "---\n",
      "Overall Perplexity Score: 2.120552660869667\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Assuming model and tokenizer are already loaded\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(...)\n",
    "\n",
    "def get_model_response(question):\n",
    "    prompt = f\"<s>[INST] {question} [/INST]\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        num_beams=1,\n",
    "        temperature=0.7,\n",
    "        top_p=0.92,\n",
    "        do_sample=True,\n",
    "        use_cache=True\n",
    "    )\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = full_response.split(\"[INST]\")[-1].split(\"[/INST]\")[-1].strip()\n",
    "    \n",
    "    # Extract only the relevant part of the answer\n",
    "    if \"प्रश्न:\" in response:\n",
    "        response = response.split(\"प्रश्न:\")[-1].strip()\n",
    "    if \"उत्तर:\" in response:\n",
    "        response = response.split(\"उत्तर:\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "def calculate_perplexity(question, answer):\n",
    "    inputs = tokenizer(question + \" \" + answer, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    return torch.exp(outputs.loss).item()\n",
    "\n",
    "# Load the dataset\n",
    "with open('Hindi_Benchmark_data.json', 'r', encoding='utf-8') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "perplexities = []\n",
    "\n",
    "# Limit to first 20 questions\n",
    "for item in dataset[:20]:\n",
    "    question = item[\"Question\"]\n",
    "    reference_answer = item[\"Answer\"]\n",
    "    \n",
    "    model_answer = get_model_response(question)\n",
    "    \n",
    "    perplexity = calculate_perplexity(question, model_answer)\n",
    "    perplexities.append(perplexity)\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Reference Answer: {reference_answer}\")\n",
    "    print(f\"Model Answer: {model_answer}\")\n",
    "    print(f\"Perplexity: {perplexity}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# Calculate the overall perplexity score\n",
    "overall_perplexity = np.exp(np.mean(np.log(perplexities)))\n",
    "\n",
    "print(f\"Overall Perplexity Score: {overall_perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
